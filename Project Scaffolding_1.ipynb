{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065cd8bd-a72f-4472-94be-da7e50ccbab3",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2ded4811-7c43-4878-9a35-bc3c971a1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_tlc_page(tlc_page_url):\n",
    "    \"\"\"\n",
    "    Extract all URLs from the NYC TLC webpage.\n",
    "\n",
    "    Parameters:\n",
    "        tlc_page_url (str): The URL of the NYC TLC webpage.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of all stripped URLs found on the page.\n",
    "    \"\"\"\n",
    "    # Fetch the webpage content\n",
    "    response = requests.get(tlc_page_url)\n",
    "    response.raise_for_status()  # Raise an error for invalid response\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract and strip all <a> tag href attributes\n",
    "    urls = [link['href'].strip() for link in soup.find_all('a', href=True)]\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c13ef0-87f6-427d-831f-0dd258b7855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_and_hvfhv_urls(all_urls):\n",
    "    \"\"\"\n",
    "    Filter URLs for Yellow Taxi and HVFHV Parquet files for the years 2020–2024.\n",
    "\n",
    "    Parameters:\n",
    "        all_urls (list): A list of URLs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of Yellow Taxi URLs, list of HVFHV URLs)\n",
    "    \"\"\"\n",
    "    taxi_pattern = re.compile(r\".*yellow_tripdata_20(20|21|22|23|24)-.*\\.parquet$\", re.IGNORECASE)\n",
    "    hvfhv_pattern = re.compile(r\".*fhvhv_tripdata_20(20|21|22|23|24)-.*\\.parquet$\", re.IGNORECASE)\n",
    "\n",
    "    yellow_taxi_urls = [url for url in all_urls if taxi_pattern.search(url)]\n",
    "    hvfhv_urls = [url for url in all_urls if hvfhv_pattern.search(url)]\n",
    "\n",
    "    return yellow_taxi_urls, hvfhv_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Download Taxi and HVFHV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "89e24255-0df4-4285-8f51-90f5ba37c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_yellow_taxi_data(tlc_page_url, save_dir=\"yellow_taxi_data\"):\n",
    "    \"\"\"\n",
    "    Download all Yellow Taxi Parquet files for the years 2020–2024.\n",
    "\n",
    "    Parameters:\n",
    "        tlc_page_url (str): URL of the TLC page containing data links.\n",
    "        save_dir (str): Directory to save Yellow Taxi data files.\n",
    "    \"\"\"\n",
    "    # Fetch and process URLs\n",
    "    all_urls = get_all_urls_from_tlc_page(tlc_page_url)\n",
    "    base_url = \"https://www1.nyc.gov\"\n",
    "    all_urls = convert_to_absolute_urls(all_urls, base_url)\n",
    "    yellow_taxi_urls, _ = filter_taxi_and_hvfhv_urls(all_urls)\n",
    "\n",
    "    print(f\"Found {len(yellow_taxi_urls)} Yellow Taxi Parquet files.\")\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for url in yellow_taxi_urls:\n",
    "        download_parquet_file(url, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "34ecf19f-1c94-439d-ab47-133deac422ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57 Yellow Taxi Parquet files.\n",
      "Downloaded yellow_tripdata_2024-01.parquet to yellow_taxi_data/yellow_tripdata_2024-01.parquet\n",
      "Downloaded yellow_tripdata_2024-02.parquet to yellow_taxi_data/yellow_tripdata_2024-02.parquet\n",
      "Downloaded yellow_tripdata_2024-03.parquet to yellow_taxi_data/yellow_tripdata_2024-03.parquet\n",
      "Downloaded yellow_tripdata_2024-04.parquet to yellow_taxi_data/yellow_tripdata_2024-04.parquet\n",
      "Downloaded yellow_tripdata_2024-05.parquet to yellow_taxi_data/yellow_tripdata_2024-05.parquet\n",
      "Downloaded yellow_tripdata_2024-06.parquet to yellow_taxi_data/yellow_tripdata_2024-06.parquet\n",
      "Downloaded yellow_tripdata_2024-07.parquet to yellow_taxi_data/yellow_tripdata_2024-07.parquet\n",
      "Downloaded yellow_tripdata_2024-08.parquet to yellow_taxi_data/yellow_tripdata_2024-08.parquet\n",
      "Downloaded yellow_tripdata_2024-09.parquet to yellow_taxi_data/yellow_tripdata_2024-09.parquet\n",
      "Downloaded yellow_tripdata_2023-01.parquet to yellow_taxi_data/yellow_tripdata_2023-01.parquet\n",
      "Downloaded yellow_tripdata_2023-02.parquet to yellow_taxi_data/yellow_tripdata_2023-02.parquet\n",
      "Downloaded yellow_tripdata_2023-03.parquet to yellow_taxi_data/yellow_tripdata_2023-03.parquet\n",
      "Downloaded yellow_tripdata_2023-04.parquet to yellow_taxi_data/yellow_tripdata_2023-04.parquet\n",
      "Downloaded yellow_tripdata_2023-05.parquet to yellow_taxi_data/yellow_tripdata_2023-05.parquet\n",
      "Downloaded yellow_tripdata_2023-06.parquet to yellow_taxi_data/yellow_tripdata_2023-06.parquet\n",
      "Downloaded yellow_tripdata_2023-07.parquet to yellow_taxi_data/yellow_tripdata_2023-07.parquet\n",
      "Downloaded yellow_tripdata_2023-08.parquet to yellow_taxi_data/yellow_tripdata_2023-08.parquet\n",
      "Downloaded yellow_tripdata_2023-09.parquet to yellow_taxi_data/yellow_tripdata_2023-09.parquet\n",
      "Downloaded yellow_tripdata_2023-10.parquet to yellow_taxi_data/yellow_tripdata_2023-10.parquet\n",
      "Downloaded yellow_tripdata_2023-11.parquet to yellow_taxi_data/yellow_tripdata_2023-11.parquet\n",
      "Downloaded yellow_tripdata_2023-12.parquet to yellow_taxi_data/yellow_tripdata_2023-12.parquet\n",
      "Downloaded yellow_tripdata_2022-01.parquet to yellow_taxi_data/yellow_tripdata_2022-01.parquet\n",
      "Downloaded yellow_tripdata_2022-02.parquet to yellow_taxi_data/yellow_tripdata_2022-02.parquet\n",
      "Downloaded yellow_tripdata_2022-03.parquet to yellow_taxi_data/yellow_tripdata_2022-03.parquet\n",
      "Downloaded yellow_tripdata_2022-04.parquet to yellow_taxi_data/yellow_tripdata_2022-04.parquet\n",
      "Downloaded yellow_tripdata_2022-05.parquet to yellow_taxi_data/yellow_tripdata_2022-05.parquet\n",
      "Downloaded yellow_tripdata_2022-06.parquet to yellow_taxi_data/yellow_tripdata_2022-06.parquet\n",
      "Downloaded yellow_tripdata_2022-07.parquet to yellow_taxi_data/yellow_tripdata_2022-07.parquet\n",
      "Downloaded yellow_tripdata_2022-08.parquet to yellow_taxi_data/yellow_tripdata_2022-08.parquet\n",
      "Downloaded yellow_tripdata_2022-09.parquet to yellow_taxi_data/yellow_tripdata_2022-09.parquet\n",
      "Downloaded yellow_tripdata_2022-10.parquet to yellow_taxi_data/yellow_tripdata_2022-10.parquet\n",
      "Downloaded yellow_tripdata_2022-11.parquet to yellow_taxi_data/yellow_tripdata_2022-11.parquet\n",
      "Downloaded yellow_tripdata_2022-12.parquet to yellow_taxi_data/yellow_tripdata_2022-12.parquet\n",
      "Downloaded yellow_tripdata_2021-01.parquet to yellow_taxi_data/yellow_tripdata_2021-01.parquet\n",
      "Downloaded yellow_tripdata_2021-02.parquet to yellow_taxi_data/yellow_tripdata_2021-02.parquet\n",
      "Downloaded yellow_tripdata_2021-03.parquet to yellow_taxi_data/yellow_tripdata_2021-03.parquet\n",
      "Downloaded yellow_tripdata_2021-04.parquet to yellow_taxi_data/yellow_tripdata_2021-04.parquet\n",
      "Downloaded yellow_tripdata_2021-05.parquet to yellow_taxi_data/yellow_tripdata_2021-05.parquet\n",
      "Downloaded yellow_tripdata_2021-06.parquet to yellow_taxi_data/yellow_tripdata_2021-06.parquet\n",
      "Downloaded yellow_tripdata_2021-07.parquet to yellow_taxi_data/yellow_tripdata_2021-07.parquet\n",
      "Downloaded yellow_tripdata_2021-08.parquet to yellow_taxi_data/yellow_tripdata_2021-08.parquet\n",
      "Downloaded yellow_tripdata_2021-09.parquet to yellow_taxi_data/yellow_tripdata_2021-09.parquet\n",
      "Downloaded yellow_tripdata_2021-10.parquet to yellow_taxi_data/yellow_tripdata_2021-10.parquet\n",
      "Downloaded yellow_tripdata_2021-11.parquet to yellow_taxi_data/yellow_tripdata_2021-11.parquet\n",
      "Downloaded yellow_tripdata_2021-12.parquet to yellow_taxi_data/yellow_tripdata_2021-12.parquet\n",
      "Downloaded yellow_tripdata_2020-01.parquet to yellow_taxi_data/yellow_tripdata_2020-01.parquet\n",
      "Downloaded yellow_tripdata_2020-02.parquet to yellow_taxi_data/yellow_tripdata_2020-02.parquet\n",
      "Downloaded yellow_tripdata_2020-03.parquet to yellow_taxi_data/yellow_tripdata_2020-03.parquet\n",
      "Downloaded yellow_tripdata_2020-04.parquet to yellow_taxi_data/yellow_tripdata_2020-04.parquet\n",
      "Downloaded yellow_tripdata_2020-05.parquet to yellow_taxi_data/yellow_tripdata_2020-05.parquet\n",
      "Downloaded yellow_tripdata_2020-06.parquet to yellow_taxi_data/yellow_tripdata_2020-06.parquet\n",
      "Downloaded yellow_tripdata_2020-07.parquet to yellow_taxi_data/yellow_tripdata_2020-07.parquet\n",
      "Downloaded yellow_tripdata_2020-08.parquet to yellow_taxi_data/yellow_tripdata_2020-08.parquet\n",
      "Downloaded yellow_tripdata_2020-09.parquet to yellow_taxi_data/yellow_tripdata_2020-09.parquet\n",
      "Downloaded yellow_tripdata_2020-10.parquet to yellow_taxi_data/yellow_tripdata_2020-10.parquet\n",
      "Downloaded yellow_tripdata_2020-11.parquet to yellow_taxi_data/yellow_tripdata_2020-11.parquet\n",
      "Downloaded yellow_tripdata_2020-12.parquet to yellow_taxi_data/yellow_tripdata_2020-12.parquet\n"
     ]
    }
   ],
   "source": [
    "# TLC page URL\n",
    "tlc_page_url = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# Download Yellow Taxi data\n",
    "download_yellow_taxi_data(tlc_page_url, save_dir=\"yellow_taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "42a4f9b2-3f9e-45f6-bdaa-04acf478ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hvfhv_data(tlc_page_url, save_dir=\"hvfhv_data\"):\n",
    "    \"\"\"\n",
    "    Download all HVFHV Parquet files for the years 2020–2024.\n",
    "\n",
    "    Parameters:\n",
    "        tlc_page_url (str): URL of the TLC page containing data links.\n",
    "        save_dir (str): Directory to save HVFHV data files.\n",
    "    \"\"\"\n",
    "    # Fetch and process URLs\n",
    "    all_urls = get_all_urls_from_tlc_page(tlc_page_url)\n",
    "    base_url = \"https://www1.nyc.gov\"\n",
    "    all_urls = convert_to_absolute_urls(all_urls, base_url)\n",
    "    _, hvfhv_urls = filter_taxi_and_hvfhv_urls(all_urls)\n",
    "\n",
    "    print(f\"Found {len(hvfhv_urls)} HVFHV Parquet files.\")\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for url in hvfhv_urls:\n",
    "        download_parquet_file(url, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "65598984-f2e2-46ee-9452-2f82a9343576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57 HVFHV Parquet files.\n",
      "Downloaded fhvhv_tripdata_2024-01.parquet to hvfhv_data/fhvhv_tripdata_2024-01.parquet\n",
      "Downloaded fhvhv_tripdata_2024-02.parquet to hvfhv_data/fhvhv_tripdata_2024-02.parquet\n",
      "Downloaded fhvhv_tripdata_2024-03.parquet to hvfhv_data/fhvhv_tripdata_2024-03.parquet\n",
      "Downloaded fhvhv_tripdata_2024-04.parquet to hvfhv_data/fhvhv_tripdata_2024-04.parquet\n",
      "Downloaded fhvhv_tripdata_2024-05.parquet to hvfhv_data/fhvhv_tripdata_2024-05.parquet\n",
      "Downloaded fhvhv_tripdata_2024-06.parquet to hvfhv_data/fhvhv_tripdata_2024-06.parquet\n",
      "Downloaded fhvhv_tripdata_2024-07.parquet to hvfhv_data/fhvhv_tripdata_2024-07.parquet\n",
      "Downloaded fhvhv_tripdata_2024-08.parquet to hvfhv_data/fhvhv_tripdata_2024-08.parquet\n",
      "Downloaded fhvhv_tripdata_2024-09.parquet to hvfhv_data/fhvhv_tripdata_2024-09.parquet\n",
      "Downloaded fhvhv_tripdata_2023-01.parquet to hvfhv_data/fhvhv_tripdata_2023-01.parquet\n",
      "Downloaded fhvhv_tripdata_2023-02.parquet to hvfhv_data/fhvhv_tripdata_2023-02.parquet\n",
      "Downloaded fhvhv_tripdata_2023-03.parquet to hvfhv_data/fhvhv_tripdata_2023-03.parquet\n",
      "Downloaded fhvhv_tripdata_2023-04.parquet to hvfhv_data/fhvhv_tripdata_2023-04.parquet\n",
      "Downloaded fhvhv_tripdata_2023-05.parquet to hvfhv_data/fhvhv_tripdata_2023-05.parquet\n",
      "Downloaded fhvhv_tripdata_2023-06.parquet to hvfhv_data/fhvhv_tripdata_2023-06.parquet\n",
      "Downloaded fhvhv_tripdata_2023-07.parquet to hvfhv_data/fhvhv_tripdata_2023-07.parquet\n",
      "Downloaded fhvhv_tripdata_2023-08.parquet to hvfhv_data/fhvhv_tripdata_2023-08.parquet\n",
      "Downloaded fhvhv_tripdata_2023-09.parquet to hvfhv_data/fhvhv_tripdata_2023-09.parquet\n",
      "Downloaded fhvhv_tripdata_2023-10.parquet to hvfhv_data/fhvhv_tripdata_2023-10.parquet\n",
      "Downloaded fhvhv_tripdata_2023-11.parquet to hvfhv_data/fhvhv_tripdata_2023-11.parquet\n",
      "Downloaded fhvhv_tripdata_2023-12.parquet to hvfhv_data/fhvhv_tripdata_2023-12.parquet\n",
      "Downloaded fhvhv_tripdata_2022-01.parquet to hvfhv_data/fhvhv_tripdata_2022-01.parquet\n",
      "Downloaded fhvhv_tripdata_2022-02.parquet to hvfhv_data/fhvhv_tripdata_2022-02.parquet\n",
      "Downloaded fhvhv_tripdata_2022-03.parquet to hvfhv_data/fhvhv_tripdata_2022-03.parquet\n",
      "Downloaded fhvhv_tripdata_2022-04.parquet to hvfhv_data/fhvhv_tripdata_2022-04.parquet\n",
      "Downloaded fhvhv_tripdata_2022-05.parquet to hvfhv_data/fhvhv_tripdata_2022-05.parquet\n",
      "Downloaded fhvhv_tripdata_2022-06.parquet to hvfhv_data/fhvhv_tripdata_2022-06.parquet\n",
      "Downloaded fhvhv_tripdata_2022-07.parquet to hvfhv_data/fhvhv_tripdata_2022-07.parquet\n",
      "Downloaded fhvhv_tripdata_2022-08.parquet to hvfhv_data/fhvhv_tripdata_2022-08.parquet\n",
      "Downloaded fhvhv_tripdata_2022-09.parquet to hvfhv_data/fhvhv_tripdata_2022-09.parquet\n",
      "Downloaded fhvhv_tripdata_2022-10.parquet to hvfhv_data/fhvhv_tripdata_2022-10.parquet\n",
      "Downloaded fhvhv_tripdata_2022-11.parquet to hvfhv_data/fhvhv_tripdata_2022-11.parquet\n",
      "Downloaded fhvhv_tripdata_2022-12.parquet to hvfhv_data/fhvhv_tripdata_2022-12.parquet\n",
      "Downloaded fhvhv_tripdata_2021-01.parquet to hvfhv_data/fhvhv_tripdata_2021-01.parquet\n",
      "Downloaded fhvhv_tripdata_2021-02.parquet to hvfhv_data/fhvhv_tripdata_2021-02.parquet\n",
      "Downloaded fhvhv_tripdata_2021-03.parquet to hvfhv_data/fhvhv_tripdata_2021-03.parquet\n",
      "Downloaded fhvhv_tripdata_2021-04.parquet to hvfhv_data/fhvhv_tripdata_2021-04.parquet\n",
      "Downloaded fhvhv_tripdata_2021-05.parquet to hvfhv_data/fhvhv_tripdata_2021-05.parquet\n",
      "Downloaded fhvhv_tripdata_2021-06.parquet to hvfhv_data/fhvhv_tripdata_2021-06.parquet\n",
      "Downloaded fhvhv_tripdata_2021-07.parquet to hvfhv_data/fhvhv_tripdata_2021-07.parquet\n",
      "Downloaded fhvhv_tripdata_2021-08.parquet to hvfhv_data/fhvhv_tripdata_2021-08.parquet\n",
      "Downloaded fhvhv_tripdata_2021-09.parquet to hvfhv_data/fhvhv_tripdata_2021-09.parquet\n",
      "Downloaded fhvhv_tripdata_2021-10.parquet to hvfhv_data/fhvhv_tripdata_2021-10.parquet\n",
      "Downloaded fhvhv_tripdata_2021-11.parquet to hvfhv_data/fhvhv_tripdata_2021-11.parquet\n",
      "Downloaded fhvhv_tripdata_2021-12.parquet to hvfhv_data/fhvhv_tripdata_2021-12.parquet\n",
      "Downloaded fhvhv_tripdata_2020-01.parquet to hvfhv_data/fhvhv_tripdata_2020-01.parquet\n",
      "Downloaded fhvhv_tripdata_2020-02.parquet to hvfhv_data/fhvhv_tripdata_2020-02.parquet\n",
      "Downloaded fhvhv_tripdata_2020-03.parquet to hvfhv_data/fhvhv_tripdata_2020-03.parquet\n",
      "Downloaded fhvhv_tripdata_2020-04.parquet to hvfhv_data/fhvhv_tripdata_2020-04.parquet\n",
      "Downloaded fhvhv_tripdata_2020-05.parquet to hvfhv_data/fhvhv_tripdata_2020-05.parquet\n",
      "Downloaded fhvhv_tripdata_2020-06.parquet to hvfhv_data/fhvhv_tripdata_2020-06.parquet\n",
      "Downloaded fhvhv_tripdata_2020-07.parquet to hvfhv_data/fhvhv_tripdata_2020-07.parquet\n",
      "Downloaded fhvhv_tripdata_2020-08.parquet to hvfhv_data/fhvhv_tripdata_2020-08.parquet\n",
      "Downloaded fhvhv_tripdata_2020-09.parquet to hvfhv_data/fhvhv_tripdata_2020-09.parquet\n",
      "Downloaded fhvhv_tripdata_2020-10.parquet to hvfhv_data/fhvhv_tripdata_2020-10.parquet\n",
      "Downloaded fhvhv_tripdata_2020-11.parquet to hvfhv_data/fhvhv_tripdata_2020-11.parquet\n",
      "Downloaded fhvhv_tripdata_2020-12.parquet to hvfhv_data/fhvhv_tripdata_2020-12.parquet\n"
     ]
    }
   ],
   "source": [
    "# TLC page URL\n",
    "tlc_page_url = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# Download HVFHV data\n",
    "download_hvfhv_data(tlc_page_url, save_dir=\"hvfhv_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea79981a-f056-4611-b937-d0609536041b",
   "metadata": {},
   "source": [
    "## Sampleing Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b6f3f02-ea8e-47ee-87c5-ef08519e5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cochran_sample_size(population, confidence_level=0.95, margin_of_error=0.05, proportion=0.5):\n",
    "    \"\"\"\n",
    "    Calculate sample size using Cochran's formula with finite population correction.\n",
    "\n",
    "    Parameters:\n",
    "        population (int): Total population size (e.g., number of records for a month).\n",
    "        confidence_level (float): Desired confidence level (e.g., 0.95 for 95%).\n",
    "        margin_of_error (float): Desired margin of error (e.g., 0.05 for 5%).\n",
    "        proportion (float): Estimated proportion of the population (default 0.5 for max variability).\n",
    "\n",
    "    Returns:\n",
    "        int: Sample size.\n",
    "    \"\"\"\n",
    "    # Z-score for given confidence level\n",
    "    z_scores = {0.9: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "    z = z_scores[confidence_level]\n",
    "\n",
    "    # Initial sample size calculation\n",
    "    n0 = (z**2 * proportion * (1 - proportion)) / (margin_of_error**2)\n",
    "\n",
    "    # Apply finite population correction\n",
    "    if population > 0:\n",
    "        n = n0 / (1 + (n0 - 1) / population)\n",
    "    else:\n",
    "        n = n0\n",
    "\n",
    "    return math.ceil(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceda0a1-d646-46b8-9452-39d988784203",
   "metadata": {},
   "source": [
    "## Calculate sample size for taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4c95e2be-2d11-4e2a-a477-ac04223a606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: yellow_tripdata_2023-06.parquet\n",
      "Population size: 3307234, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-06.parquet\n",
      "Processing file: yellow_tripdata_2022-10.parquet\n",
      "Population size: 3675411, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-10.parquet\n",
      "Processing file: yellow_tripdata_2020-03.parquet\n",
      "Population size: 3007687, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-03.parquet\n",
      "Processing file: yellow_tripdata_2021-05.parquet\n",
      "Population size: 2507109, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-05.parquet\n",
      "Processing file: yellow_tripdata_2022-09.parquet\n",
      "Population size: 3183767, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-09.parquet\n",
      "Processing file: yellow_tripdata_2024-04.parquet\n",
      "Population size: 3514289, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-04.parquet\n",
      "Processing file: yellow_tripdata_2020-12.parquet\n",
      "Population size: 1461898, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-12.parquet\n",
      "Processing file: yellow_tripdata_2020-02.parquet\n",
      "Population size: 6299367, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-02.parquet\n",
      "Processing file: yellow_tripdata_2021-04.parquet\n",
      "Population size: 2171187, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-04.parquet\n",
      "Processing file: yellow_tripdata_2022-08.parquet\n",
      "Population size: 3152677, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-08.parquet\n",
      "Processing file: yellow_tripdata_2024-05.parquet\n",
      "Population size: 3723833, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-05.parquet\n",
      "Processing file: yellow_tripdata_2023-07.parquet\n",
      "Population size: 2907108, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-07.parquet\n",
      "Processing file: yellow_tripdata_2022-01.parquet\n",
      "Population size: 2463931, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-01.parquet\n",
      "Processing file: yellow_tripdata_2022-11.parquet\n",
      "Population size: 3252717, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-11.parquet\n",
      "Processing file: yellow_tripdata_2020-10.parquet\n",
      "Population size: 1681132, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-10.parquet\n",
      "Processing file: yellow_tripdata_2024-07.parquet\n",
      "Population size: 3076903, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-07.parquet\n",
      "Processing file: yellow_tripdata_2021-06.parquet\n",
      "Population size: 2834264, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-06.parquet\n",
      "Processing file: yellow_tripdata_2023-05.parquet\n",
      "Population size: 3513649, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-05.parquet\n",
      "Processing file: yellow_tripdata_2020-09.parquet\n",
      "Population size: 1341017, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-09.parquet\n",
      "Processing file: yellow_tripdata_2022-03.parquet\n",
      "Population size: 3627882, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-03.parquet\n",
      "Processing file: yellow_tripdata_2023-04.parquet\n",
      "Population size: 3288250, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-04.parquet\n",
      "Processing file: yellow_tripdata_2020-08.parquet\n",
      "Population size: 1007286, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-08.parquet\n",
      "Processing file: yellow_tripdata_2022-02.parquet\n",
      "Population size: 2979431, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-02.parquet\n",
      "Processing file: yellow_tripdata_2022-12.parquet\n",
      "Population size: 3399549, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-12.parquet\n",
      "Processing file: yellow_tripdata_2020-11.parquet\n",
      "Population size: 1509000, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-11.parquet\n",
      "Processing file: yellow_tripdata_2020-01.parquet\n",
      "Population size: 6405008, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-01.parquet\n",
      "Processing file: yellow_tripdata_2024-06.parquet\n",
      "Population size: 3539193, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-06.parquet\n",
      "Processing file: yellow_tripdata_2021-07.parquet\n",
      "Population size: 2821746, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-07.parquet\n",
      "Processing file: yellow_tripdata_2024-03.parquet\n",
      "Population size: 3582628, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-03.parquet\n",
      "Processing file: yellow_tripdata_2021-02.parquet\n",
      "Population size: 1371709, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-02.parquet\n",
      "Processing file: yellow_tripdata_2021-12.parquet\n",
      "Population size: 3214369, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-12.parquet\n",
      "Processing file: yellow_tripdata_2020-04.parquet\n",
      "Population size: 238073, Sample size: 662\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-04.parquet\n",
      "Processing file: yellow_tripdata_2023-08.parquet\n",
      "Population size: 2824209, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-08.parquet\n",
      "Processing file: yellow_tripdata_2022-07.parquet\n",
      "Population size: 3174394, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-07.parquet\n",
      "Processing file: yellow_tripdata_2023-11.parquet\n",
      "Population size: 3339715, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-11.parquet\n",
      "Processing file: yellow_tripdata_2023-01.parquet\n",
      "Population size: 3066766, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-01.parquet\n",
      "Processing file: yellow_tripdata_2022-06.parquet\n",
      "Population size: 3558124, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-06.parquet\n",
      "Processing file: yellow_tripdata_2023-10.parquet\n",
      "Population size: 3522285, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-10.parquet\n",
      "Processing file: yellow_tripdata_2024-02.parquet\n",
      "Population size: 3007526, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-02.parquet\n",
      "Processing file: yellow_tripdata_2021-03.parquet\n",
      "Population size: 1925152, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-03.parquet\n",
      "Processing file: yellow_tripdata_2020-05.parquet\n",
      "Population size: 348415, Sample size: 663\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-05.parquet\n",
      "Processing file: yellow_tripdata_2023-09.parquet\n",
      "Population size: 2846722, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-09.parquet\n",
      "Processing file: yellow_tripdata_2024-09.parquet\n",
      "Population size: 3633030, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-09.parquet\n",
      "Processing file: yellow_tripdata_2022-04.parquet\n",
      "Population size: 3599920, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-04.parquet\n",
      "Processing file: yellow_tripdata_2021-08.parquet\n",
      "Population size: 2788757, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-08.parquet\n",
      "Processing file: yellow_tripdata_2023-12.parquet\n",
      "Population size: 3376567, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-12.parquet\n",
      "Processing file: yellow_tripdata_2023-02.parquet\n",
      "Population size: 2913955, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-02.parquet\n",
      "Processing file: yellow_tripdata_2021-01.parquet\n",
      "Population size: 1369769, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-01.parquet\n",
      "Processing file: yellow_tripdata_2021-11.parquet\n",
      "Population size: 3472949, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-11.parquet\n",
      "Processing file: yellow_tripdata_2020-07.parquet\n",
      "Population size: 800412, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-07.parquet\n",
      "Processing file: yellow_tripdata_2021-10.parquet\n",
      "Population size: 3463504, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-10.parquet\n",
      "Processing file: yellow_tripdata_2024-01.parquet\n",
      "Population size: 2964624, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-01.parquet\n",
      "Processing file: yellow_tripdata_2020-06.parquet\n",
      "Population size: 549797, Sample size: 663\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-06.parquet\n",
      "Processing file: yellow_tripdata_2024-08.parquet\n",
      "Population size: 2979183, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-08.parquet\n",
      "Processing file: yellow_tripdata_2022-05.parquet\n",
      "Population size: 3588295, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-05.parquet\n",
      "Processing file: yellow_tripdata_2021-09.parquet\n",
      "Population size: 2963793, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-09.parquet\n",
      "Processing file: yellow_tripdata_2023-03.parquet\n",
      "Population size: 3403766, Sample size: 664\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-03.parquet\n"
     ]
    }
   ],
   "source": [
    "# Directory containing cleaned files\n",
    "input_dir = \"yellow_taxi_data\"\n",
    "output_dir = \"sampled_taxi_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "# Sampling parameters\n",
    "confidence_level = 0.99\n",
    "margin_of_error = 0.05\n",
    "proportion = 0.5\n",
    "\n",
    "# Process each file\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        # Load the monthly data\n",
    "        monthly_data = pd.read_parquet(file_path)\n",
    "        population_size = len(monthly_data)\n",
    "        \n",
    "        # Calculate the sample size for the month\n",
    "        sample_size = cochran_sample_size(population_size, confidence_level, margin_of_error, proportion)\n",
    "        print(f\"Population size: {population_size}, Sample size: {sample_size}\")\n",
    "        \n",
    "        # Perform sampling\n",
    "        sampled_data = monthly_data.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Save the sampled data\n",
    "        output_file = os.path.join(output_dir, f\"sampled_{file_name}\")\n",
    "        sampled_data.to_parquet(output_file)\n",
    "        print(f\"Saved sampled data to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157b194-6733-494d-9b0e-c26b9fa33df1",
   "metadata": {},
   "source": [
    "## Set sample size = 664 for taxi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5aa715ae-f4f6-48b7-bd80-650a3d32cd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: yellow_tripdata_2023-06.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-06.parquet\n",
      "Processing file: yellow_tripdata_2022-10.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-10.parquet\n",
      "Processing file: yellow_tripdata_2020-03.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-03.parquet\n",
      "Processing file: yellow_tripdata_2021-05.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-05.parquet\n",
      "Processing file: yellow_tripdata_2022-09.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-09.parquet\n",
      "Processing file: yellow_tripdata_2024-04.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-04.parquet\n",
      "Processing file: yellow_tripdata_2020-12.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-12.parquet\n",
      "Processing file: yellow_tripdata_2020-02.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-02.parquet\n",
      "Processing file: yellow_tripdata_2021-04.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-04.parquet\n",
      "Processing file: yellow_tripdata_2022-08.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-08.parquet\n",
      "Processing file: yellow_tripdata_2024-05.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-05.parquet\n",
      "Processing file: yellow_tripdata_2023-07.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-07.parquet\n",
      "Processing file: yellow_tripdata_2022-01.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-01.parquet\n",
      "Processing file: yellow_tripdata_2022-11.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-11.parquet\n",
      "Processing file: yellow_tripdata_2020-10.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-10.parquet\n",
      "Processing file: yellow_tripdata_2024-07.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-07.parquet\n",
      "Processing file: yellow_tripdata_2021-06.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-06.parquet\n",
      "Processing file: yellow_tripdata_2023-05.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-05.parquet\n",
      "Processing file: yellow_tripdata_2020-09.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-09.parquet\n",
      "Processing file: yellow_tripdata_2022-03.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-03.parquet\n",
      "Processing file: yellow_tripdata_2023-04.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-04.parquet\n",
      "Processing file: yellow_tripdata_2020-08.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-08.parquet\n",
      "Processing file: yellow_tripdata_2022-02.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-02.parquet\n",
      "Processing file: yellow_tripdata_2022-12.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-12.parquet\n",
      "Processing file: yellow_tripdata_2020-11.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-11.parquet\n",
      "Processing file: yellow_tripdata_2020-01.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-01.parquet\n",
      "Processing file: yellow_tripdata_2024-06.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-06.parquet\n",
      "Processing file: yellow_tripdata_2021-07.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-07.parquet\n",
      "Processing file: yellow_tripdata_2024-03.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-03.parquet\n",
      "Processing file: yellow_tripdata_2021-02.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-02.parquet\n",
      "Processing file: yellow_tripdata_2021-12.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-12.parquet\n",
      "Processing file: yellow_tripdata_2020-04.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-04.parquet\n",
      "Processing file: yellow_tripdata_2023-08.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-08.parquet\n",
      "Processing file: yellow_tripdata_2022-07.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-07.parquet\n",
      "Processing file: yellow_tripdata_2023-11.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-11.parquet\n",
      "Processing file: yellow_tripdata_2023-01.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-01.parquet\n",
      "Processing file: yellow_tripdata_2022-06.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-06.parquet\n",
      "Processing file: yellow_tripdata_2023-10.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-10.parquet\n",
      "Processing file: yellow_tripdata_2024-02.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-02.parquet\n",
      "Processing file: yellow_tripdata_2021-03.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-03.parquet\n",
      "Processing file: yellow_tripdata_2020-05.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-05.parquet\n",
      "Processing file: yellow_tripdata_2023-09.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-09.parquet\n",
      "Processing file: yellow_tripdata_2024-09.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-09.parquet\n",
      "Processing file: yellow_tripdata_2022-04.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-04.parquet\n",
      "Processing file: yellow_tripdata_2021-08.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-08.parquet\n",
      "Processing file: yellow_tripdata_2023-12.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-12.parquet\n",
      "Processing file: yellow_tripdata_2023-02.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-02.parquet\n",
      "Processing file: yellow_tripdata_2021-01.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-01.parquet\n",
      "Processing file: yellow_tripdata_2021-11.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-11.parquet\n",
      "Processing file: yellow_tripdata_2020-07.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-07.parquet\n",
      "Processing file: yellow_tripdata_2021-10.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-10.parquet\n",
      "Processing file: yellow_tripdata_2024-01.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-01.parquet\n",
      "Processing file: yellow_tripdata_2020-06.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2020-06.parquet\n",
      "Processing file: yellow_tripdata_2024-08.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2024-08.parquet\n",
      "Processing file: yellow_tripdata_2022-05.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2022-05.parquet\n",
      "Processing file: yellow_tripdata_2021-09.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2021-09.parquet\n",
      "Processing file: yellow_tripdata_2023-03.parquet\n",
      "Saved sampled data to: sampled_taxi_data/sampled_yellow_tripdata_2023-03.parquet\n"
     ]
    }
   ],
   "source": [
    "fixed_sample_size = 664  # we set 664 as the desired sample size \n",
    "\n",
    "input_dir = \"yellow_taxi_data\"\n",
    "output_dir = \"sampled_taxi_data\"\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        # Load the monthly data\n",
    "        monthly_data = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Perform sampling\n",
    "        sampled_data = monthly_data.sample(n=fixed_sample_size, random_state=42)\n",
    "        \n",
    "        # Save the sampled data\n",
    "        output_file = os.path.join(output_dir, f\"sampled_{file_name}\")\n",
    "        sampled_data.to_parquet(output_file)\n",
    "        print(f\"Saved sampled data to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4915b-9b62-4f4e-bc7b-b56615977a79",
   "metadata": {},
   "source": [
    "## Calculate Sample size for HVFHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "403aedf5-c415-402f-94e1-aff014c26bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: fhvhv_tripdata_2021-03.parquet\n",
      "Population size: 14227393, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-03.parquet\n",
      "Processing file: fhvhv_tripdata_2024-02.parquet\n",
      "Population size: 19359148, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-02.parquet\n",
      "Processing file: fhvhv_tripdata_2023-09.parquet\n",
      "Population size: 19851123, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-09.parquet\n",
      "Processing file: fhvhv_tripdata_2020-05.parquet\n",
      "Population size: 6089999, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-05.parquet\n",
      "Processing file: fhvhv_tripdata_2022-06.parquet\n",
      "Population size: 17780075, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-06.parquet\n",
      "Processing file: fhvhv_tripdata_2023-10.parquet\n",
      "Population size: 20186330, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-10.parquet\n",
      "Processing file: fhvhv_tripdata_2022-07.parquet\n",
      "Population size: 17464619, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-07.parquet\n",
      "Processing file: fhvhv_tripdata_2023-01.parquet\n",
      "Population size: 18479031, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-01.parquet\n",
      "Processing file: fhvhv_tripdata_2023-11.parquet\n",
      "Population size: 19269250, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-11.parquet\n",
      "Processing file: fhvhv_tripdata_2021-12.parquet\n",
      "Population size: 16054495, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-12.parquet\n",
      "Processing file: fhvhv_tripdata_2021-02.parquet\n",
      "Population size: 11613942, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-02.parquet\n",
      "Processing file: fhvhv_tripdata_2024-03.parquet\n",
      "Population size: 21280788, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-03.parquet\n",
      "Processing file: fhvhv_tripdata_2023-08.parquet\n",
      "Population size: 18322150, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-08.parquet\n",
      "Processing file: fhvhv_tripdata_2020-04.parquet\n",
      "Population size: 4312909, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-04.parquet\n",
      "Processing file: fhvhv_tripdata_2021-09.parquet\n",
      "Population size: 14886055, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-09.parquet\n",
      "Processing file: fhvhv_tripdata_2022-05.parquet\n",
      "Population size: 18157335, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-05.parquet\n",
      "Processing file: fhvhv_tripdata_2024-08.parquet\n",
      "Population size: 19128392, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-08.parquet\n",
      "Processing file: fhvhv_tripdata_2023-03.parquet\n",
      "Population size: 20413539, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-03.parquet\n",
      "Processing file: fhvhv_tripdata_2024-01.parquet\n",
      "Population size: 19663930, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-01.parquet\n",
      "Processing file: fhvhv_tripdata_2021-10.parquet\n",
      "Population size: 16545356, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-10.parquet\n",
      "Processing file: fhvhv_tripdata_2020-06.parquet\n",
      "Population size: 7555193, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-06.parquet\n",
      "Processing file: fhvhv_tripdata_2021-11.parquet\n",
      "Population size: 16041639, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-11.parquet\n",
      "Processing file: fhvhv_tripdata_2021-01.parquet\n",
      "Population size: 11908468, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-01.parquet\n",
      "Processing file: fhvhv_tripdata_2020-07.parquet\n",
      "Population size: 9958454, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-07.parquet\n",
      "Processing file: fhvhv_tripdata_2021-08.parquet\n",
      "Population size: 14499696, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-08.parquet\n",
      "Processing file: fhvhv_tripdata_2022-04.parquet\n",
      "Population size: 17752561, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-04.parquet\n",
      "Processing file: fhvhv_tripdata_2024-09.parquet\n",
      "Population size: 19209788, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-09.parquet\n",
      "Processing file: fhvhv_tripdata_2023-02.parquet\n",
      "Population size: 17960971, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-02.parquet\n",
      "Processing file: fhvhv_tripdata_2023-12.parquet\n",
      "Population size: 20516297, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-12.parquet\n",
      "Processing file: fhvhv_tripdata_2023-07.parquet\n",
      "Population size: 19132131, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-07.parquet\n",
      "Processing file: fhvhv_tripdata_2022-11.parquet\n",
      "Population size: 18085896, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-11.parquet\n",
      "Processing file: fhvhv_tripdata_2022-01.parquet\n",
      "Population size: 14751591, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-01.parquet\n",
      "Processing file: fhvhv_tripdata_2020-02.parquet\n",
      "Population size: 21725100, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-02.parquet\n",
      "Processing file: fhvhv_tripdata_2020-12.parquet\n",
      "Population size: 11637123, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-12.parquet\n",
      "Processing file: fhvhv_tripdata_2024-05.parquet\n",
      "Population size: 20704538, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-05.parquet\n",
      "Processing file: fhvhv_tripdata_2022-08.parquet\n",
      "Population size: 17185687, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-08.parquet\n",
      "Processing file: fhvhv_tripdata_2021-04.parquet\n",
      "Population size: 14111371, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-04.parquet\n",
      "Processing file: fhvhv_tripdata_2020-03.parquet\n",
      "Population size: 13392928, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-03.parquet\n",
      "Processing file: fhvhv_tripdata_2024-04.parquet\n",
      "Population size: 19733038, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-04.parquet\n",
      "Processing file: fhvhv_tripdata_2022-09.parquet\n",
      "Population size: 17793551, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-09.parquet\n",
      "Processing file: fhvhv_tripdata_2021-05.parquet\n",
      "Population size: 14719171, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-05.parquet\n",
      "Processing file: fhvhv_tripdata_2023-06.parquet\n",
      "Population size: 19366619, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-06.parquet\n",
      "Processing file: fhvhv_tripdata_2022-10.parquet\n",
      "Population size: 19306090, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-10.parquet\n",
      "Processing file: fhvhv_tripdata_2020-01.parquet\n",
      "Population size: 20569368, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-01.parquet\n",
      "Processing file: fhvhv_tripdata_2020-11.parquet\n",
      "Population size: 11596865, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-11.parquet\n",
      "Processing file: fhvhv_tripdata_2021-07.parquet\n",
      "Population size: 15027174, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-07.parquet\n",
      "Processing file: fhvhv_tripdata_2024-06.parquet\n",
      "Population size: 20123226, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-06.parquet\n",
      "Processing file: fhvhv_tripdata_2020-08.parquet\n",
      "Population size: 11096852, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-08.parquet\n",
      "Processing file: fhvhv_tripdata_2023-04.parquet\n",
      "Population size: 19144903, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-04.parquet\n",
      "Processing file: fhvhv_tripdata_2022-12.parquet\n",
      "Population size: 19665847, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-12.parquet\n",
      "Processing file: fhvhv_tripdata_2022-02.parquet\n",
      "Population size: 16019283, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-02.parquet\n",
      "Processing file: fhvhv_tripdata_2020-09.parquet\n",
      "Population size: 12106669, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-09.parquet\n",
      "Processing file: fhvhv_tripdata_2023-05.parquet\n",
      "Population size: 19847676, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-05.parquet\n",
      "Processing file: fhvhv_tripdata_2022-03.parquet\n",
      "Population size: 18453548, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-03.parquet\n",
      "Processing file: fhvhv_tripdata_2020-10.parquet\n",
      "Population size: 13268411, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-10.parquet\n",
      "Processing file: fhvhv_tripdata_2021-06.parquet\n",
      "Population size: 14961892, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-06.parquet\n",
      "Processing file: fhvhv_tripdata_2024-07.parquet\n",
      "Population size: 19182934, Sample size: 664\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-07.parquet\n"
     ]
    }
   ],
   "source": [
    "# Directory containing cleaned files\n",
    "input_dir = \"hvfhv_data\"\n",
    "output_dir = \"sampled_hvfhv_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "# Sampling parameters\n",
    "confidence_level = 0.99\n",
    "margin_of_error = 0.05\n",
    "proportion = 0.5\n",
    "\n",
    "# Process each file\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        # Load the monthly data\n",
    "        monthly_data = pd.read_parquet(file_path)\n",
    "        population_size = len(monthly_data)\n",
    "        \n",
    "        # Calculate the sample size for the month\n",
    "        sample_size = cochran_sample_size(population_size, confidence_level, margin_of_error, proportion)\n",
    "        print(f\"Population size: {population_size}, Sample size: {sample_size}\")\n",
    "        \n",
    "        # Perform sampling\n",
    "        sampled_data = monthly_data.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Save the sampled data\n",
    "        output_file = os.path.join(output_dir, f\"sampled_{file_name}\")\n",
    "        sampled_data.to_parquet(output_file)\n",
    "        print(f\"Saved sampled data to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d13350-1c41-4930-9fbd-5a4dfc2f56c2",
   "metadata": {},
   "source": [
    "## Set sample size = 664 for HVFHV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "676d45c9-606b-483e-87e9-e6228753b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: fhvhv_tripdata_2021-03.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-03.parquet\n",
      "Processing file: fhvhv_tripdata_2024-02.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-02.parquet\n",
      "Processing file: fhvhv_tripdata_2023-09.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-09.parquet\n",
      "Processing file: fhvhv_tripdata_2020-05.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-05.parquet\n",
      "Processing file: fhvhv_tripdata_2022-06.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-06.parquet\n",
      "Processing file: fhvhv_tripdata_2023-10.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-10.parquet\n",
      "Processing file: fhvhv_tripdata_2022-07.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-07.parquet\n",
      "Processing file: fhvhv_tripdata_2023-01.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-01.parquet\n",
      "Processing file: fhvhv_tripdata_2023-11.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-11.parquet\n",
      "Processing file: fhvhv_tripdata_2021-12.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-12.parquet\n",
      "Processing file: fhvhv_tripdata_2021-02.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-02.parquet\n",
      "Processing file: fhvhv_tripdata_2024-03.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-03.parquet\n",
      "Processing file: fhvhv_tripdata_2023-08.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-08.parquet\n",
      "Processing file: fhvhv_tripdata_2020-04.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-04.parquet\n",
      "Processing file: fhvhv_tripdata_2021-09.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-09.parquet\n",
      "Processing file: fhvhv_tripdata_2022-05.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-05.parquet\n",
      "Processing file: fhvhv_tripdata_2024-08.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-08.parquet\n",
      "Processing file: fhvhv_tripdata_2023-03.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-03.parquet\n",
      "Processing file: fhvhv_tripdata_2024-01.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-01.parquet\n",
      "Processing file: fhvhv_tripdata_2021-10.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-10.parquet\n",
      "Processing file: fhvhv_tripdata_2020-06.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-06.parquet\n",
      "Processing file: fhvhv_tripdata_2021-11.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-11.parquet\n",
      "Processing file: fhvhv_tripdata_2021-01.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-01.parquet\n",
      "Processing file: fhvhv_tripdata_2020-07.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-07.parquet\n",
      "Processing file: fhvhv_tripdata_2021-08.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-08.parquet\n",
      "Processing file: fhvhv_tripdata_2022-04.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-04.parquet\n",
      "Processing file: fhvhv_tripdata_2024-09.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-09.parquet\n",
      "Processing file: fhvhv_tripdata_2023-02.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-02.parquet\n",
      "Processing file: fhvhv_tripdata_2023-12.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-12.parquet\n",
      "Processing file: fhvhv_tripdata_2023-07.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-07.parquet\n",
      "Processing file: fhvhv_tripdata_2022-11.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-11.parquet\n",
      "Processing file: fhvhv_tripdata_2022-01.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-01.parquet\n",
      "Processing file: fhvhv_tripdata_2020-02.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-02.parquet\n",
      "Processing file: fhvhv_tripdata_2020-12.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-12.parquet\n",
      "Processing file: fhvhv_tripdata_2024-05.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-05.parquet\n",
      "Processing file: fhvhv_tripdata_2022-08.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-08.parquet\n",
      "Processing file: fhvhv_tripdata_2021-04.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-04.parquet\n",
      "Processing file: fhvhv_tripdata_2020-03.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-03.parquet\n",
      "Processing file: fhvhv_tripdata_2024-04.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-04.parquet\n",
      "Processing file: fhvhv_tripdata_2022-09.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-09.parquet\n",
      "Processing file: fhvhv_tripdata_2021-05.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-05.parquet\n",
      "Processing file: fhvhv_tripdata_2023-06.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-06.parquet\n",
      "Processing file: fhvhv_tripdata_2022-10.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-10.parquet\n",
      "Processing file: fhvhv_tripdata_2020-01.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-01.parquet\n",
      "Processing file: fhvhv_tripdata_2020-11.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-11.parquet\n",
      "Processing file: fhvhv_tripdata_2021-07.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-07.parquet\n",
      "Processing file: fhvhv_tripdata_2024-06.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-06.parquet\n",
      "Processing file: fhvhv_tripdata_2020-08.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-08.parquet\n",
      "Processing file: fhvhv_tripdata_2023-04.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-04.parquet\n",
      "Processing file: fhvhv_tripdata_2022-12.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-12.parquet\n",
      "Processing file: fhvhv_tripdata_2022-02.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-02.parquet\n",
      "Processing file: fhvhv_tripdata_2020-09.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-09.parquet\n",
      "Processing file: fhvhv_tripdata_2023-05.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2023-05.parquet\n",
      "Processing file: fhvhv_tripdata_2022-03.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2022-03.parquet\n",
      "Processing file: fhvhv_tripdata_2020-10.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2020-10.parquet\n",
      "Processing file: fhvhv_tripdata_2021-06.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2021-06.parquet\n",
      "Processing file: fhvhv_tripdata_2024-07.parquet\n",
      "Saved sampled data to: sampled_hvfhv_data/sampled_fhvhv_tripdata_2024-07.parquet\n"
     ]
    }
   ],
   "source": [
    "fixed_sample_size = 664  # we set 64 as the desired sample size \n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        # Load the monthly data\n",
    "        monthly_data = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Perform sampling\n",
    "        sampled_data = monthly_data.sample(n=fixed_sample_size, random_state=42)\n",
    "        \n",
    "        # Save the sampled data\n",
    "        output_file = os.path.join(output_dir, f\"sampled_{file_name}\")\n",
    "        sampled_data.to_parquet(output_file)\n",
    "        print(f\"Saved sampled data to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Filtering Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8c4353d7-30a9-48a9-baf8-5be031ac6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_uber_by_license(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Filter Uber rides from HVFHV dataset using 'Hvfhs_license_num'.\n",
    "\n",
    "    Parameters:\n",
    "        input_dir (str): Directory containing raw HVFHV Parquet files.\n",
    "        output_dir (str): Directory to save filtered Uber rides.\n",
    "\n",
    "    Returns:\n",
    "        None: Saves filtered Uber rides to the output directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith(\".parquet\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Load the HVFHV data\n",
    "            hvfhv_data = pd.read_parquet(file_path)\n",
    "\n",
    "            # Filter for Uber rides where Hvfhs_license_num is 'HV0003'\n",
    "            uber_data = hvfhv_data[hvfhv_data['hvfhs_license_num'] == 'HV0003']\n",
    "\n",
    "            # Save the filtered data\n",
    "            output_file = os.path.join(output_dir, f\"uber_{file_name}\")\n",
    "            uber_data.to_parquet(output_file)\n",
    "            print(f\"Saved filtered Uber data to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8f23a43a-c197-4b89-a2a9-21b439644940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: sampled_fhvhv_tripdata_2022-01.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-01.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2022-11.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-11.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-07.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-07.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2024-05.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2024-05.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2022-08.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-08.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-04.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-04.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-12.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-12.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-02.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-02.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2024-04.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2024-04.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2022-09.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-09.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-05.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-05.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-03.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-03.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2022-10.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-10.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-06.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-06.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-07.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-07.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2024-06.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2024-06.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-11.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-11.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-01.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-01.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2022-02.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-02.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2022-12.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-12.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-08.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-08.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-04.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-04.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2022-03.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-03.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-09.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-09.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-05.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-05.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-06.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-06.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2024-07.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2024-07.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-10.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-10.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-09.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-09.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-05.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-05.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-03.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-03.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2024-02.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2024-02.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-10.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-10.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2022-06.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-06.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-11.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-11.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-01.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-01.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2022-07.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-07.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-08.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-08.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-04.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-04.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-02.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-02.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-12.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-12.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2024-03.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2024-03.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-03.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-03.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-09.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-09.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2022-05.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-05.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2024-08.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2024-08.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-06.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-06.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2024-01.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2024-01.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-10.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-10.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2020-07.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2020-07.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-01.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-01.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-11.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-11.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-12.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-12.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2023-02.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2023-02.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2021-08.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2021-08.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2022-04.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2022-04.parquet\n",
      "Processing file: sampled_fhvhv_tripdata_2024-09.parquet\n",
      "Saved filtered Uber data to: uber_data/uber_sampled_fhvhv_tripdata_2024-09.parquet\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"sampled_hvfhv_data\"\n",
    "output_dir = \"uber_data\"\n",
    "\n",
    "# Filter Uber rides\n",
    "filter_uber_by_license(hvfhv_input_dir, uber_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722dc7cd-c642-4025-bac6-8f3a218647b5",
   "metadata": {},
   "source": [
    "## Data Cleaning for taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "17c5bb0d-536b-4bae-9c33-ca811b027022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rc/sh_trsb91r31xqwtp6547r_r0000gn/T/ipykernel_15008/4125480126.py:11: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  zones_gdf['centroid'] = zones_gdf.geometry.centroid\n",
      "/var/folders/rc/sh_trsb91r31xqwtp6547r_r0000gn/T/ipykernel_15008/4125480126.py:12: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  zones_gdf['latitude'] = zones_gdf.centroid.y\n",
      "/var/folders/rc/sh_trsb91r31xqwtp6547r_r0000gn/T/ipykernel_15008/4125480126.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  zones_gdf['longitude'] = zones_gdf.centroid.x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-03.parquet\n",
      "File sampled_yellow_tripdata_2020-03.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2024-04.parquet\n",
      "File sampled_yellow_tripdata_2024-04.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-05.parquet\n",
      "File sampled_yellow_tripdata_2021-05.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-09.parquet\n",
      "File sampled_yellow_tripdata_2022-09.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-06.parquet\n",
      "File sampled_yellow_tripdata_2023-06.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-10.parquet\n",
      "File sampled_yellow_tripdata_2022-10.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-07.parquet\n",
      "File sampled_yellow_tripdata_2023-07.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-01.parquet\n",
      "File sampled_yellow_tripdata_2022-01.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-11.parquet\n",
      "File sampled_yellow_tripdata_2022-11.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-12.parquet\n",
      "File sampled_yellow_tripdata_2020-12.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-02.parquet\n",
      "File sampled_yellow_tripdata_2020-02.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2024-05.parquet\n",
      "File sampled_yellow_tripdata_2024-05.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-04.parquet\n",
      "File sampled_yellow_tripdata_2021-04.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-08.parquet\n",
      "File sampled_yellow_tripdata_2022-08.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-05.parquet\n",
      "File sampled_yellow_tripdata_2023-05.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-09.parquet\n",
      "File sampled_yellow_tripdata_2020-09.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-03.parquet\n",
      "File sampled_yellow_tripdata_2022-03.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-10.parquet\n",
      "File sampled_yellow_tripdata_2020-10.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-06.parquet\n",
      "File sampled_yellow_tripdata_2021-06.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2024-07.parquet\n",
      "File sampled_yellow_tripdata_2024-07.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-11.parquet\n",
      "File sampled_yellow_tripdata_2020-11.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-01.parquet\n",
      "File sampled_yellow_tripdata_2020-01.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-07.parquet\n",
      "File sampled_yellow_tripdata_2021-07.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2024-06.parquet\n",
      "File sampled_yellow_tripdata_2024-06.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-04.parquet\n",
      "File sampled_yellow_tripdata_2023-04.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-08.parquet\n",
      "File sampled_yellow_tripdata_2020-08.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-02.parquet\n",
      "File sampled_yellow_tripdata_2022-02.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-12.parquet\n",
      "File sampled_yellow_tripdata_2022-12.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-07.parquet\n",
      "File sampled_yellow_tripdata_2022-07.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-11.parquet\n",
      "File sampled_yellow_tripdata_2023-11.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-01.parquet\n",
      "File sampled_yellow_tripdata_2023-01.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-02.parquet\n",
      "File sampled_yellow_tripdata_2021-02.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-12.parquet\n",
      "File sampled_yellow_tripdata_2021-12.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2024-03.parquet\n",
      "File sampled_yellow_tripdata_2024-03.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-04.parquet\n",
      "File sampled_yellow_tripdata_2020-04.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-08.parquet\n",
      "File sampled_yellow_tripdata_2023-08.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-03.parquet\n",
      "File sampled_yellow_tripdata_2021-03.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2024-02.parquet\n",
      "File sampled_yellow_tripdata_2024-02.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-05.parquet\n",
      "File sampled_yellow_tripdata_2020-05.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-09.parquet\n",
      "File sampled_yellow_tripdata_2023-09.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-06.parquet\n",
      "File sampled_yellow_tripdata_2022-06.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-10.parquet\n",
      "File sampled_yellow_tripdata_2023-10.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-01.parquet\n",
      "File sampled_yellow_tripdata_2021-01.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-11.parquet\n",
      "File sampled_yellow_tripdata_2021-11.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-07.parquet\n",
      "File sampled_yellow_tripdata_2020-07.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-04.parquet\n",
      "File sampled_yellow_tripdata_2022-04.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-08.parquet\n",
      "File sampled_yellow_tripdata_2021-08.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2024-09.parquet\n",
      "File sampled_yellow_tripdata_2024-09.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-12.parquet\n",
      "File sampled_yellow_tripdata_2023-12.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-02.parquet\n",
      "File sampled_yellow_tripdata_2023-02.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2022-05.parquet\n",
      "File sampled_yellow_tripdata_2022-05.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-09.parquet\n",
      "File sampled_yellow_tripdata_2021-09.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2024-08.parquet\n",
      "File sampled_yellow_tripdata_2024-08.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2023-03.parquet\n",
      "File sampled_yellow_tripdata_2023-03.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2024-01.parquet\n",
      "File sampled_yellow_tripdata_2024-01.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2021-10.parquet\n",
      "File sampled_yellow_tripdata_2021-10.parquet processed and saved.\n",
      "Processing file: sampled_taxi_data/sampled_yellow_tripdata_2020-06.parquet\n",
      "File sampled_yellow_tripdata_2020-06.parquet processed and saved.\n",
      "All files have been processed and consolidated.\n"
     ]
    }
   ],
   "source": [
    "# Define directories\n",
    "input_directory = \"sampled_taxi_data/\"\n",
    "output_directory = \"Cleaned_Taxi_Data/\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Load taxi zone shapefile\n",
    "zones_gdf = gpd.read_file('taxi_zones')  # Replace with your actual path\n",
    "zones_gdf = zones_gdf.to_crs(epsg=4326)  # Ensure CRS is WGS84 for latitude and longitude\n",
    "\n",
    "# Calculate centroids for pickup and dropoff locations\n",
    "zones_gdf['centroid'] = zones_gdf.geometry.centroid\n",
    "zones_gdf['latitude'] = zones_gdf.centroid.y\n",
    "zones_gdf['longitude'] = zones_gdf.centroid.x\n",
    "zones_df = zones_gdf[['LocationID', 'latitude', 'longitude']]\n",
    "\n",
    "# Latitude and longitude bounds for NYC\n",
    "LAT_MIN, LAT_MAX = 40.4774, 40.9176\n",
    "LON_MIN, LON_MAX = -74.2591, -73.7004\n",
    "\n",
    "def clean_taxi_data(file_path):\n",
    "    # Read the Parquet file\n",
    "    trips_df = pd.read_parquet(file_path)\n",
    "    print('Processing file:', file_path)\n",
    "\n",
    "    # Merge trip data with zone centroids for pickups\n",
    "    trips_with_pickup = trips_df.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='PULocationID',\n",
    "        right_on='LocationID'\n",
    "    ).rename(columns={'latitude': 'pickup_latitude', 'longitude': 'pickup_longitude'})\n",
    "\n",
    "    # Merge trip data with zone centroids for dropoffs\n",
    "    trips_with_locations = trips_with_pickup.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='DOLocationID',\n",
    "        right_on='LocationID',\n",
    "        suffixes=('', '_dropoff')\n",
    "    ).rename(columns={'latitude': 'dropoff_latitude', 'longitude': 'dropoff_longitude'})\n",
    "\n",
    "    # Filter out trips with invalid location IDs\n",
    "    valid_trips = trips_with_locations.dropna(subset=['pickup_latitude', 'dropoff_latitude'])\n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['pickup_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['pickup_longitude'].between(LON_MIN, LON_MAX)) &\n",
    "        (valid_trips['dropoff_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['dropoff_longitude'].between(LON_MIN, LON_MAX))\n",
    "    ]\n",
    "\n",
    "    # Drop original LocationID columns\n",
    "    valid_trips.drop(['PULocationID', 'DOLocationID', 'LocationID', 'LocationID_dropoff'], axis=1, inplace=True)\n",
    "\n",
    "    # Convert column names to lowercase\n",
    "    valid_trips.columns = valid_trips.columns.str.lower()\n",
    "\n",
    "    # Filter out trips with non-positive or missing trip distances\n",
    "    valid_trips = valid_trips.dropna(subset=['trip_distance'])\n",
    "    valid_trips = valid_trips[valid_trips['trip_distance'] > 0]\n",
    "    valid_trips['trip_distance'] = valid_trips['trip_distance'].astype(float)\n",
    "\n",
    "    # Filter out trips with non-positive or missing passenger counts\n",
    "    valid_trips = valid_trips.dropna(subset=['passenger_count'])\n",
    "    valid_trips = valid_trips[valid_trips['passenger_count'] > 0]\n",
    "    valid_trips['passenger_count'] = valid_trips['passenger_count'].astype(int)\n",
    "\n",
    "    # Filter out trips with negative fare amounts\n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['fare_amount'] >= 0) &\n",
    "        (valid_trips['total_amount'] >= 0) &\n",
    "        (valid_trips['tolls_amount'] >= 0)\n",
    "    ]\n",
    "\n",
    "    # Filter out trips with invalid payment types\n",
    "    valid_trips['payment_type'] = valid_trips['payment_type'].astype(int)\n",
    "    valid_trips = valid_trips[valid_trips['payment_type'].between(1, 6)]\n",
    "\n",
    "    # Filter out trips with invalid RateCodeID values\n",
    "    valid_trips['ratecodeid'] = valid_trips['ratecodeid'].astype(int)\n",
    "    valid_trips = valid_trips[valid_trips['ratecodeid'].between(1, 6)]\n",
    "\n",
    "    # Convert store_and_fwd_flag to binary\n",
    "    valid_trips['store_and_fwd_flag'] = valid_trips['store_and_fwd_flag'].map({'Y': 1, 'N': 0}).fillna(0)\n",
    "\n",
    "    # Convert airport_fee to float\n",
    "    valid_trips['airport_fee'] = pd.to_numeric(valid_trips['airport_fee'], errors='coerce').fillna(0)\n",
    "\n",
    "    # Rename columns\n",
    "    valid_trips = valid_trips.rename(\n",
    "        columns={'extra': 'Miscellaneous_Extras', 'tpep_pickup_datetime': 'pickup_datetime', 'tpep_dropoff_datetime': 'dropoff_datetime'}\n",
    "    )\n",
    "\n",
    "    # Filter out trips where dropoff is earlier than pickup\n",
    "    valid_trips = valid_trips[valid_trips['dropoff_datetime'] >= valid_trips['pickup_datetime']]\n",
    "\n",
    "    return valid_trips\n",
    "\n",
    "# Process all files in the input directory\n",
    "all_cleaned_data = pd.DataFrame()\n",
    "taxi_file_names = [f for f in os.listdir(input_directory) if f.endswith('.parquet')]\n",
    "\n",
    "for file in taxi_file_names:\n",
    "    file_path = os.path.join(input_directory, file)\n",
    "    cleaned_data = clean_taxi_data(file_path)\n",
    "    output_file = os.path.join(output_directory, file)\n",
    "    cleaned_data.to_parquet(output_file)\n",
    "    all_cleaned_data = pd.concat([all_cleaned_data, cleaned_data], axis=0)\n",
    "    print(f'File {file} processed and saved.')\n",
    "\n",
    "# Save the consolidated cleaned data\n",
    "final_output_file = os.path.join(output_directory, 'Taxi_all.parquet')\n",
    "all_cleaned_data.to_parquet(final_output_file)\n",
    "print('All files have been processed and consolidated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "9c0ad9a7-5025-40e2-9bb6-5f2224ac3ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendorid</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>ratecodeid</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>Miscellaneous_Extras</th>\n",
       "      <th>...</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773</td>\n",
       "      <td>33773</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "      <td>33773.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.720576</td>\n",
       "      <td>2022-05-14 04:58:48.261599744</td>\n",
       "      <td>2022-05-14 05:14:45.448405760</td>\n",
       "      <td>1.428656</td>\n",
       "      <td>3.153477</td>\n",
       "      <td>1.044473</td>\n",
       "      <td>0.009593</td>\n",
       "      <td>1.230954</td>\n",
       "      <td>15.024451</td>\n",
       "      <td>1.261126</td>\n",
       "      <td>...</td>\n",
       "      <td>2.790389</td>\n",
       "      <td>0.416870</td>\n",
       "      <td>0.559586</td>\n",
       "      <td>22.302831</td>\n",
       "      <td>2.335223</td>\n",
       "      <td>0.084513</td>\n",
       "      <td>40.753638</td>\n",
       "      <td>-73.967629</td>\n",
       "      <td>40.755853</td>\n",
       "      <td>-73.971790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2020-01-01 00:11:06</td>\n",
       "      <td>2020-01-01 00:30:50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.525495</td>\n",
       "      <td>-74.233534</td>\n",
       "      <td>40.525495</td>\n",
       "      <td>-74.233534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2021-03-15 11:32:47</td>\n",
       "      <td>2021-03-15 12:02:01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.740439</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2022-05-14 20:06:07</td>\n",
       "      <td>2022-05-14 20:16:51</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>16.630000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2023-07-13 19:09:09</td>\n",
       "      <td>2023-07-13 19:30:28</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.880000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.965146</td>\n",
       "      <td>40.774376</td>\n",
       "      <td>-73.959635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2024-09-30 23:52:50</td>\n",
       "      <td>2024-09-30 23:56:08</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>73.190000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>422.700000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>453.550000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.711026</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.711026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.448723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971209</td>\n",
       "      <td>4.037154</td>\n",
       "      <td>0.277529</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>0.459047</td>\n",
       "      <td>13.716804</td>\n",
       "      <td>1.526065</td>\n",
       "      <td>...</td>\n",
       "      <td>3.935447</td>\n",
       "      <td>1.756412</td>\n",
       "      <td>0.338178</td>\n",
       "      <td>17.813367</td>\n",
       "      <td>0.620324</td>\n",
       "      <td>0.349249</td>\n",
       "      <td>0.030732</td>\n",
       "      <td>0.044780</td>\n",
       "      <td>0.031435</td>\n",
       "      <td>0.034966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           vendorid                pickup_datetime  \\\n",
       "count  33773.000000                          33773   \n",
       "mean       1.720576  2022-05-14 04:58:48.261599744   \n",
       "min        1.000000            2020-01-01 00:11:06   \n",
       "25%        1.000000            2021-03-15 11:32:47   \n",
       "50%        2.000000            2022-05-14 20:06:07   \n",
       "75%        2.000000            2023-07-13 19:09:09   \n",
       "max        2.000000            2024-09-30 23:52:50   \n",
       "std        0.448723                            NaN   \n",
       "\n",
       "                    dropoff_datetime  passenger_count  trip_distance  \\\n",
       "count                          33773     33773.000000   33773.000000   \n",
       "mean   2022-05-14 05:14:45.448405760         1.428656       3.153477   \n",
       "min              2020-01-01 00:30:50         1.000000       0.010000   \n",
       "25%              2021-03-15 12:02:01         1.000000       1.060000   \n",
       "50%              2022-05-14 20:16:51         1.000000       1.750000   \n",
       "75%              2023-07-13 19:30:28         1.000000       3.170000   \n",
       "max              2024-09-30 23:56:08         6.000000      73.190000   \n",
       "std                              NaN         0.971209       4.037154   \n",
       "\n",
       "         ratecodeid  store_and_fwd_flag  payment_type   fare_amount  \\\n",
       "count  33773.000000        33773.000000  33773.000000  33773.000000   \n",
       "mean       1.044473            0.009593      1.230954     15.024451   \n",
       "min        1.000000            0.000000      1.000000      0.000000   \n",
       "25%        1.000000            0.000000      1.000000      7.200000   \n",
       "50%        1.000000            0.000000      1.000000     10.500000   \n",
       "75%        1.000000            0.000000      1.000000     16.500000   \n",
       "max        5.000000            1.000000      4.000000    228.000000   \n",
       "std        0.277529            0.097477      0.459047     13.716804   \n",
       "\n",
       "       Miscellaneous_Extras  ...    tip_amount  tolls_amount  \\\n",
       "count          33773.000000  ...  33773.000000  33773.000000   \n",
       "mean               1.261126  ...      2.790389      0.416870   \n",
       "min                0.000000  ...      0.000000      0.000000   \n",
       "25%                0.000000  ...      0.050000      0.000000   \n",
       "50%                0.500000  ...      2.200000      0.000000   \n",
       "75%                2.500000  ...      3.550000      0.000000   \n",
       "max               11.750000  ...    422.700000     40.000000   \n",
       "std                1.526065  ...      3.935447      1.756412   \n",
       "\n",
       "       improvement_surcharge  total_amount  congestion_surcharge  \\\n",
       "count           33773.000000  33773.000000          33773.000000   \n",
       "mean                0.559586     22.302831              2.335223   \n",
       "min                 0.000000      0.000000              0.000000   \n",
       "25%                 0.300000     12.600000              2.500000   \n",
       "50%                 0.300000     16.630000              2.500000   \n",
       "75%                 1.000000     23.880000              2.500000   \n",
       "max                 1.000000    453.550000              2.500000   \n",
       "std                 0.338178     17.813367              0.620324   \n",
       "\n",
       "        airport_fee  pickup_latitude  pickup_longitude  dropoff_latitude  \\\n",
       "count  33773.000000     33773.000000      33773.000000      33773.000000   \n",
       "mean       0.084513        40.753638        -73.967629         40.755853   \n",
       "min        0.000000        40.525495        -74.233534         40.525495   \n",
       "25%        0.000000        40.740439        -73.989845         40.740337   \n",
       "50%        0.000000        40.758028        -73.977698         40.758028   \n",
       "75%        0.000000        40.773633        -73.965146         40.774376   \n",
       "max        1.750000        40.899529        -73.711026         40.899529   \n",
       "std        0.349249         0.030732          0.044780          0.031435   \n",
       "\n",
       "       dropoff_longitude  \n",
       "count       33773.000000  \n",
       "mean          -73.971790  \n",
       "min           -74.233534  \n",
       "25%           -73.989845  \n",
       "50%           -73.977698  \n",
       "75%           -73.959635  \n",
       "max           -73.711026  \n",
       "std             0.034966  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the consolidated Parquet file\n",
    "file_path = 'Cleaned_Taxi_Data/Taxi_all.parquet'\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df_taxi = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_taxi.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3e20c47c-205e-482d-9134-f21151925137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rc/sh_trsb91r31xqwtp6547r_r0000gn/T/ipykernel_15008/3223754904.py:15: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  zones_gdf['centroid'] = zones_gdf.geometry.centroid\n",
      "/var/folders/rc/sh_trsb91r31xqwtp6547r_r0000gn/T/ipykernel_15008/3223754904.py:16: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  zones_gdf['latitude'] = zones_gdf.centroid.y\n",
      "/var/folders/rc/sh_trsb91r31xqwtp6547r_r0000gn/T/ipykernel_15008/3223754904.py:17: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  zones_gdf['longitude'] = zones_gdf.centroid.x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-07.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-07.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-01.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-01.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-11.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-11.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-12.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-12.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-02.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-02.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2024-05.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2024-05.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-08.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-08.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-04.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-04.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-03.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-03.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2024-04.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2024-04.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-09.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-09.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-05.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-05.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-06.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-06.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-10.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-10.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-11.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-11.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-01.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-01.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-07.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-07.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2024-06.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2024-06.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-08.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-08.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-04.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-04.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-02.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-02.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-12.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-12.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-09.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-09.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-05.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-05.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-03.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-03.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-10.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-10.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-06.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-06.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2024-07.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2024-07.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-03.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-03.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2024-02.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2024-02.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-09.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-09.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-05.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-05.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-06.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-06.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-10.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-10.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-07.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-07.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-11.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-11.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-01.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-01.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-02.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-02.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-12.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-12.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2024-03.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2024-03.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-08.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-08.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-04.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-04.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-09.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-09.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-05.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-05.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2024-08.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2024-08.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-03.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-03.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2024-01.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2024-01.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-10.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-10.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-06.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-06.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-01.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-01.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-11.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-11.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2020-07.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2020-07.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2021-08.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2021-08.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2022-04.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2022-04.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2024-09.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2024-09.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-12.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-12.parquet processed and saved.\n",
      "Processing file: uber_data/uber_sampled_fhvhv_tripdata_2023-02.parquet\n",
      "File uber_sampled_fhvhv_tripdata_2023-02.parquet processed and saved.\n",
      "All files have been processed and consolidated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Define directories\n",
    "input_directory = \"uber_data/\"\n",
    "output_directory = \"Cleaned_Uber_Data/\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Load taxi zone shapefile\n",
    "zones_gdf = gpd.read_file('taxi_zones')  # Replace with your actual path\n",
    "zones_gdf = zones_gdf.to_crs(epsg=4326)  # Ensure CRS is WGS84 for latitude and longitude\n",
    "\n",
    "# Calculate centroids for pickup and dropoff locations\n",
    "zones_gdf['centroid'] = zones_gdf.geometry.centroid\n",
    "zones_gdf['latitude'] = zones_gdf.centroid.y\n",
    "zones_gdf['longitude'] = zones_gdf.centroid.x\n",
    "zones_df = zones_gdf[['LocationID', 'latitude', 'longitude']]\n",
    "\n",
    "# Latitude and longitude bounds for NYC\n",
    "LAT_MIN, LAT_MAX = 40.560445, 40.908524\n",
    "LON_MIN, LON_MAX = -74.242330, -73.717047\n",
    "\n",
    "def clean_uber_data(file_path):\n",
    "    # Read the Parquet file\n",
    "    trips_df = pd.read_parquet(file_path)\n",
    "    print('Processing file:', file_path)\n",
    "    \n",
    "    # Retain records that are Uber rides\n",
    "    trips_df = trips_df[trips_df['hvfhs_license_num'] == 'HV0003']\n",
    "\n",
    "    # Merge trip data with zone centroids for pickups\n",
    "    trips_with_pickup = trips_df.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='PULocationID',\n",
    "        right_on='LocationID'\n",
    "    ).rename(columns={'latitude': 'pickup_latitude', 'longitude': 'pickup_longitude'})\n",
    "\n",
    "    # Merge trip data with zone centroids for dropoffs\n",
    "    trips_with_locations = trips_with_pickup.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='DOLocationID',\n",
    "        right_on='LocationID',\n",
    "        suffixes=('', '_dropoff')\n",
    "    ).rename(columns={'latitude': 'dropoff_latitude', 'longitude': 'dropoff_longitude'})\n",
    "\n",
    "    # Filter out trips with invalid location IDs\n",
    "    valid_trips = trips_with_locations.dropna(subset=['pickup_latitude', 'dropoff_latitude'])\n",
    "\n",
    "    # Delete records that start_pos or end_pos is out of range\n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['pickup_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['pickup_longitude'].between(LON_MIN, LON_MAX)) &\n",
    "        (valid_trips['dropoff_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['dropoff_longitude'].between(LON_MIN, LON_MAX))\n",
    "    ]\n",
    "    \n",
    "    # Delete original LocationID columns\n",
    "    valid_trips.drop(['PULocationID', 'DOLocationID', 'LocationID', 'LocationID_dropoff'], axis=1, inplace=True)\n",
    "    \n",
    "    # Convert column names to lowercase\n",
    "    valid_trips.columns = valid_trips.columns.str.lower()\n",
    "    \n",
    "    # Delete records where trip_miles is missing or <= 0, and convert datatype into float\n",
    "    valid_trips = valid_trips.dropna(subset=['trip_miles'])\n",
    "    valid_trips = valid_trips[valid_trips['trip_miles'] > 0]\n",
    "    valid_trips['trip_miles'] = valid_trips['trip_miles'].astype(float)\n",
    "    \n",
    "    # Delete records where trip_time is missing or <= 0, and convert datatype into float\n",
    "    valid_trips = valid_trips.dropna(subset=['trip_time'])\n",
    "    valid_trips = valid_trips[valid_trips['trip_time'] > 0]\n",
    "    valid_trips['trip_time'] = valid_trips['trip_time'].astype(float)\n",
    "    \n",
    "    # Delete records where base_passenger_fare, tolls, sales_tax, bcf, tips, congestion_surcharge, or driver_pay are negative\n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['base_passenger_fare'] >= 0) &\n",
    "        (valid_trips['tolls'] >= 0) &\n",
    "        (valid_trips['sales_tax'] >= 0) &\n",
    "        (valid_trips['bcf'] >= 0) &\n",
    "        (valid_trips['tips'] >= 0) &\n",
    "        (valid_trips['congestion_surcharge'] >= 0) &\n",
    "        (valid_trips['driver_pay'] >= 0)\n",
    "    ]\n",
    "    \n",
    "    # Convert certain flags into 0 and 1\n",
    "    flag_columns = ['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_request_flag', 'wav_match_flag']\n",
    "    for col in flag_columns:\n",
    "        valid_trips[col] = valid_trips[col].map({'Y': 1, 'N': 0}).fillna(0)\n",
    "    \n",
    "    # Delete records where dropoff_datetime is earlier than pickup_datetime\n",
    "    valid_trips = valid_trips[valid_trips['dropoff_datetime'] >= valid_trips['pickup_datetime']]\n",
    "    \n",
    "    # Delete records where on_scene_datetime is earlier than request_datetime\n",
    "    valid_trips = valid_trips[valid_trips['on_scene_datetime'] >= valid_trips['request_datetime']]\n",
    "    \n",
    "    # Rename 'bcf' to 'Black_Car_Fund'\n",
    "    valid_trips = valid_trips.rename(columns={'bcf': 'Black_Car_Fund'})\n",
    "    \n",
    "    # Delete unnecessary columns\n",
    "    valid_trips = valid_trips.drop(['hvfhs_license_num', 'dispatching_base_num', 'originating_base_num'], axis=1)\n",
    "    \n",
    "    return valid_trips\n",
    "\n",
    "# Process all files in the input directory\n",
    "all_cleaned_data = pd.DataFrame()\n",
    "uber_file_names = [f for f in os.listdir(input_directory) if f.endswith('.parquet')]\n",
    "\n",
    "for file in uber_file_names:\n",
    "    file_path = os.path.join(input_directory, file)\n",
    "    cleaned_data = clean_uber_data(file_path)\n",
    "    output_file = os.path.join(output_directory, file)\n",
    "    cleaned_data.to_parquet(output_file)\n",
    "    all_cleaned_data = pd.concat([all_cleaned_data, cleaned_data], axis=0)\n",
    "    print(f'File {file} processed and saved.')\n",
    "\n",
    "# Save the consolidated cleaned data\n",
    "final_output_file = os.path.join(output_directory, 'Uber_all.parquet')\n",
    "all_cleaned_data.to_parquet(final_output_file)\n",
    "print('All files have been processed and consolidated.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "f7c8bffc-c716-4e91-9838-fea68c668e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_datetime</th>\n",
       "      <th>on_scene_datetime</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>tolls</th>\n",
       "      <th>Black_Car_Fund</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>...</th>\n",
       "      <th>driver_pay</th>\n",
       "      <th>shared_request_flag</th>\n",
       "      <th>shared_match_flag</th>\n",
       "      <th>access_a_ride_flag</th>\n",
       "      <th>wav_request_flag</th>\n",
       "      <th>wav_match_flag</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26339</td>\n",
       "      <td>26339</td>\n",
       "      <td>26339</td>\n",
       "      <td>26339</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "      <td>26339.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-18 10:37:28.525760256</td>\n",
       "      <td>2022-05-18 10:41:09.112912384</td>\n",
       "      <td>2022-05-18 10:42:14.951972096</td>\n",
       "      <td>2022-05-18 11:00:06.771365632</td>\n",
       "      <td>4.382245</td>\n",
       "      <td>1071.832833</td>\n",
       "      <td>21.071663</td>\n",
       "      <td>0.664221</td>\n",
       "      <td>0.616029</td>\n",
       "      <td>1.878243</td>\n",
       "      <td>...</td>\n",
       "      <td>16.940955</td>\n",
       "      <td>0.021565</td>\n",
       "      <td>0.009074</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.062455</td>\n",
       "      <td>40.737396</td>\n",
       "      <td>-73.934473</td>\n",
       "      <td>40.737116</td>\n",
       "      <td>-73.934870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 03:15:18</td>\n",
       "      <td>2020-01-01 03:24:58</td>\n",
       "      <td>2020-01-01 03:27:51</td>\n",
       "      <td>2020-01-01 03:30:21</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.170887</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.186419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-03-04 15:12:59</td>\n",
       "      <td>2021-03-04 15:16:29</td>\n",
       "      <td>2021-03-04 15:16:40.500000</td>\n",
       "      <td>2021-03-04 15:50:35</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>557.000000</td>\n",
       "      <td>10.535000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.320000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.690787</td>\n",
       "      <td>-73.984196</td>\n",
       "      <td>40.690787</td>\n",
       "      <td>-73.984052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-16 04:43:12</td>\n",
       "      <td>2022-05-16 04:50:20</td>\n",
       "      <td>2022-05-16 04:50:26</td>\n",
       "      <td>2022-05-16 04:54:27</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>16.640000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>1.460000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.360000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>-73.948522</td>\n",
       "      <td>40.737699</td>\n",
       "      <td>-73.947442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-07-29 11:57:39</td>\n",
       "      <td>2023-07-29 12:01:35</td>\n",
       "      <td>2023-07-29 12:03:14</td>\n",
       "      <td>2023-07-29 12:30:24.500000</td>\n",
       "      <td>5.550000</td>\n",
       "      <td>1380.000000</td>\n",
       "      <td>26.410000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.650000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.774376</td>\n",
       "      <td>-73.899735</td>\n",
       "      <td>40.774376</td>\n",
       "      <td>-73.899536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-09-30 21:32:29</td>\n",
       "      <td>2024-09-30 21:35:32</td>\n",
       "      <td>2024-09-30 21:35:40</td>\n",
       "      <td>2024-09-30 21:50:59</td>\n",
       "      <td>35.710000</td>\n",
       "      <td>8886.000000</td>\n",
       "      <td>188.580000</td>\n",
       "      <td>43.600000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>17.570000</td>\n",
       "      <td>...</td>\n",
       "      <td>130.660000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.726655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.293501</td>\n",
       "      <td>727.302471</td>\n",
       "      <td>15.238093</td>\n",
       "      <td>2.640032</td>\n",
       "      <td>0.491771</td>\n",
       "      <td>1.417914</td>\n",
       "      <td>...</td>\n",
       "      <td>12.156578</td>\n",
       "      <td>0.145261</td>\n",
       "      <td>0.094826</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>0.043529</td>\n",
       "      <td>0.241985</td>\n",
       "      <td>0.068783</td>\n",
       "      <td>0.064737</td>\n",
       "      <td>0.069002</td>\n",
       "      <td>0.067779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    request_datetime              on_scene_datetime  \\\n",
       "count                          26339                          26339   \n",
       "mean   2022-05-18 10:37:28.525760256  2022-05-18 10:41:09.112912384   \n",
       "min              2020-01-01 03:15:18            2020-01-01 03:24:58   \n",
       "25%              2021-03-04 15:12:59            2021-03-04 15:16:29   \n",
       "50%              2022-05-16 04:43:12            2022-05-16 04:50:20   \n",
       "75%              2023-07-29 11:57:39            2023-07-29 12:01:35   \n",
       "max              2024-09-30 21:32:29            2024-09-30 21:35:32   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "                     pickup_datetime               dropoff_datetime  \\\n",
       "count                          26339                          26339   \n",
       "mean   2022-05-18 10:42:14.951972096  2022-05-18 11:00:06.771365632   \n",
       "min              2020-01-01 03:27:51            2020-01-01 03:30:21   \n",
       "25%       2021-03-04 15:16:40.500000            2021-03-04 15:50:35   \n",
       "50%              2022-05-16 04:50:26            2022-05-16 04:54:27   \n",
       "75%              2023-07-29 12:03:14     2023-07-29 12:30:24.500000   \n",
       "max              2024-09-30 21:35:40            2024-09-30 21:50:59   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "         trip_miles     trip_time  base_passenger_fare         tolls  \\\n",
       "count  26339.000000  26339.000000         26339.000000  26339.000000   \n",
       "mean       4.382245   1071.832833            21.071663      0.664221   \n",
       "min        0.010000     47.000000             0.000000      0.000000   \n",
       "25%        1.540000    557.000000            10.535000      0.000000   \n",
       "50%        2.800000    882.000000            16.640000      0.000000   \n",
       "75%        5.550000   1380.000000            26.410000      0.000000   \n",
       "max       35.710000   8886.000000           188.580000     43.600000   \n",
       "std        4.293501    727.302471            15.238093      2.640032   \n",
       "\n",
       "       Black_Car_Fund     sales_tax  ...    driver_pay  shared_request_flag  \\\n",
       "count    26339.000000  26339.000000  ...  26339.000000         26339.000000   \n",
       "mean         0.616029      1.878243  ...     16.940955             0.021565   \n",
       "min          0.000000      0.000000  ...      0.000000             0.000000   \n",
       "25%          0.290000      0.920000  ...      8.320000             0.000000   \n",
       "50%          0.470000      1.460000  ...     13.360000             0.000000   \n",
       "75%          0.770000      2.360000  ...     21.650000             0.000000   \n",
       "max          5.450000     17.570000  ...    130.660000             1.000000   \n",
       "std          0.491771      1.417914  ...     12.156578             0.145261   \n",
       "\n",
       "       shared_match_flag  access_a_ride_flag  wav_request_flag  \\\n",
       "count       26339.000000        26339.000000      26339.000000   \n",
       "mean            0.009074            0.000152          0.001898   \n",
       "min             0.000000            0.000000          0.000000   \n",
       "25%             0.000000            0.000000          0.000000   \n",
       "50%             0.000000            0.000000          0.000000   \n",
       "75%             0.000000            0.000000          0.000000   \n",
       "max             1.000000            1.000000          1.000000   \n",
       "std             0.094826            0.012323          0.043529   \n",
       "\n",
       "       wav_match_flag  pickup_latitude  pickup_longitude  dropoff_latitude  \\\n",
       "count    26339.000000     26339.000000      26339.000000      26339.000000   \n",
       "mean         0.062455        40.737396        -73.934473         40.737116   \n",
       "min          0.000000        40.561994        -74.170887         40.561994   \n",
       "25%          0.000000        40.690787        -73.984196         40.690787   \n",
       "50%          0.000000        40.736824        -73.948522         40.737699   \n",
       "75%          0.000000        40.774376        -73.899735         40.774376   \n",
       "max          1.000000        40.899529        -73.726655         40.899529   \n",
       "std          0.241985         0.068783          0.064737          0.069002   \n",
       "\n",
       "       dropoff_longitude  \n",
       "count       26339.000000  \n",
       "mean          -73.934870  \n",
       "min           -74.186419  \n",
       "25%           -73.984052  \n",
       "50%           -73.947442  \n",
       "75%           -73.899536  \n",
       "max           -73.726655  \n",
       "std             0.067779  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the consolidated Parquet file\n",
    "file_path = 'Cleaned_Uber_Data/Uber_all.parquet'\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df_uber = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_uber.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "73c659a8-3c79-48d8-9734-4e532c5c6ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['request_datetime',\n",
       " 'on_scene_datetime',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'trip_miles',\n",
       " 'trip_time',\n",
       " 'base_passenger_fare',\n",
       " 'tolls',\n",
       " 'Black_Car_Fund',\n",
       " 'sales_tax',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'tips',\n",
       " 'driver_pay',\n",
       " 'shared_request_flag',\n",
       " 'shared_match_flag',\n",
       " 'access_a_ride_flag',\n",
       " 'wav_request_flag',\n",
       " 'wav_match_flag',\n",
       " 'pickup_latitude',\n",
       " 'pickup_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'dropoff_longitude']"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_uber.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f6e64-e052-4f14-8e22-5f8ecc06da9b",
   "metadata": {},
   "source": [
    "We load weather data from CSV files stored in the Datasets/weather directory and select relevant columns related to answering the 6 queries in part III.\n",
    "Relevant columns include hourly weather conditions, such as temperature, humidity, and wind speed.\n",
    "We exclude all other columns because we believe they are not as useful in answering the questions and thus are irrelevant to our analysis.\n",
    "We split the original DATE entries into Date, Hour, and Minute.\n",
    "Data after August 2024 are excluded to correspond to the range data range of Trip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "266730e3-c109-4867-8204-7ed0da6417fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "def get_weather_from_files(file_paths: List[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads weather data from multiple CSV files, processes the data to include only relevant columns, \n",
    "    splits the 'DATE' column into 'Date' and 'Hour', and filters out rows after August 2024.\n",
    "\n",
    "    Args:\n",
    "        file_paths (List[str]): A list of file paths to the CSV files containing weather data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A single DataFrame containing the processed weather data.\n",
    "    \"\"\"\n",
    "    # Relevant columns to load\n",
    "    relevant_columns = [\n",
    "        'DATE', \n",
    "        'DailyPrecipitation', 'DailyAverageWindSpeed',\n",
    "        'DailySnowfall', 'DailySnowDepth',\n",
    "        'HourlyPrecipitation', 'HourlyWindSpeed', 'HourlyWindDirection', 'Sunset', 'Sunrise'\n",
    "    ]\n",
    "    \n",
    "    # Initialize an empty list to store dataframes\n",
    "    weather_data: List[DataFrame] = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Read the CSV, only load relevant columns\n",
    "        df = pd.read_csv(file_path, usecols=relevant_columns, low_memory=False)\n",
    "\n",
    "        # Split the 'DATE' column into date and hour\n",
    "        df[['Date', 'Hour']] = df['DATE'].str.split('T', expand=True)\n",
    "\n",
    "        # Extract hour and minute from the 'Hour' column\n",
    "        df['Minute'] = df['Hour'].str.split(':').str[1].astype(int)  # Extract minute part as integer\n",
    "        df['Hour'] = df['Hour'].str.split(':').str[0].astype(int)  # Extract and convert hour to integer\n",
    "\n",
    "        # Drop the original 'DATE' column\n",
    "        df = df.drop(columns=['DATE'])\n",
    "        \n",
    "        # Reorder columns: 'Date' and 'Hour' should be the first two columns\n",
    "        df = df[['Date', 'Hour', 'Minute'] + [col for col in df.columns if col not in ['Date', 'Hour', 'Minute']]]\n",
    "        \n",
    "        weather_data.append(df)\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    weather_df = pd.concat(weather_data, ignore_index=True)\n",
    "\n",
    "    # Filter out rows where the date is after August 2024\n",
    "    filtered_df = weather_df[weather_df['Date'] < '2024-09-01']\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Loading csv:\n",
    "file_paths = [\n",
    "    'Datasets/weather/2020_weather.csv',  \n",
    "    'Datasets/weather/2021_weather.csv',\n",
    "    'Datasets/weather/2022_weather.csv',  \n",
    "    'Datasets/weather/2023_weather.csv',\n",
    "    'Datasets/weather/2024_weather.csv'\n",
    "]\n",
    "weather_data = get_weather_from_files(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8b89c-63fe-4a2f-9c71-5f0667f7cc76",
   "metadata": {},
   "source": [
    "Generate Hourly Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "28c9e215-a316-4765-a364-fba571825cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_hourly(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Processes the weather data to select relevant columns and fill missing values.\"\"\"\n",
    "    relevant_columns = ['Date', 'Hour', 'Minute', 'HourlyPrecipitation', 'HourlyWindSpeed']\n",
    "    df = df[relevant_columns].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Replace missing values (NaN) in 'HourlyPrecipitation' with 0\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].fillna(0.00)\n",
    "\n",
    "    # Replace missing values in 'HourlyWindSpeed' with the mean\n",
    "    df['HourlyWindSpeed'] = df['HourlyWindSpeed'].fillna(df['HourlyWindSpeed'].mean())\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "48694596-d8c6-4beb-b2af-04f9f76f695b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54587</th>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54588</th>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>22</td>\n",
       "      <td>51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54589</th>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>23</td>\n",
       "      <td>51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54590</th>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>23</td>\n",
       "      <td>59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.148104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54591</th>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>23</td>\n",
       "      <td>59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.148104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54592 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  Hour  Minute HourlyPrecipitation  HourlyWindSpeed\n",
       "0      2020-01-01     0      51                0.00         8.000000\n",
       "1      2020-01-01     1      51                0.00         8.000000\n",
       "2      2020-01-01     2      51                0.00        14.000000\n",
       "3      2020-01-01     3      51                0.00        11.000000\n",
       "4      2020-01-01     4      51                0.00         6.000000\n",
       "...           ...   ...     ...                 ...              ...\n",
       "54587  2024-08-31    22       5                 0.0         0.000000\n",
       "54588  2024-08-31    22      51                0.00         0.000000\n",
       "54589  2024-08-31    23      51                0.00         0.000000\n",
       "54590  2024-08-31    23      59                 0.0         5.148104\n",
       "54591  2024-08-31    23      59                 0.0         5.148104\n",
       "\n",
       "[54592 rows x 5 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_hour = weather_hourly(weather_data)\n",
    "weather_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_weather_hourly(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Aggregates hourly weather data by date and hour, and fills missing values.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert 'HourlyPrecipitation' to numeric if it's still an object\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "\n",
    "    # Fill missing wind speed values with the daily mean wind speed\n",
    "    df['HourlyWindSpeed'] = df.groupby('Date')['HourlyWindSpeed'].transform(\n",
    "        lambda x: x.fillna(x.mean())\n",
    "    )\n",
    "\n",
    "    # Aggregate the data\n",
    "    aggregated_df = df.groupby(['Date', 'Hour'], as_index=False).agg({\n",
    "        'HourlyPrecipitation': 'sum',  # Sum of precipitation for the hour\n",
    "        'HourlyWindSpeed': 'mean',  # Mean wind speed for the hour\n",
    "    })\n",
    "\n",
    "    # Ensure there are no NaN values in the aggregated dataframe\n",
    "    aggregated_df['HourlyPrecipitation'] = aggregated_df['HourlyPrecipitation'].fillna(0.0)\n",
    "    aggregated_df['HourlyWindSpeed'] = aggregated_df['HourlyWindSpeed'].fillna(\n",
    "        aggregated_df['HourlyWindSpeed'].mean()\n",
    "    )\n",
    "\n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_hourly = aggregate_weather_hourly(weather_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "93dd96b6-eb78-4eab-a353-0e1e27b471fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40905 entries, 0 to 40904\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Date                 40905 non-null  object \n",
      " 1   Hour                 40905 non-null  int64  \n",
      " 2   HourlyPrecipitation  40905 non-null  float64\n",
      " 3   HourlyWindSpeed      40905 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "aggregated_hourly.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db678950-794d-435e-8008-051864fdf604",
   "metadata": {},
   "source": [
    "Generate Daily Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_daily(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Processes daily weather data to select relevant columns.\"\"\"\n",
    "    relevant_columns = ['Date', 'DailyAverageWindSpeed', 'DailyPrecipitation', 'DailySnowDepth', 'DailySnowfall']\n",
    "    return df[relevant_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2fde2ca7-dd83-4e42-894f-a6641215948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather_daily = weather_daily(weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "31679887-1ed7-4d5e-a55a-3cd3a27400e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 54592 entries, 0 to 54591\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Date                   54592 non-null  object \n",
      " 1   DailyAverageWindSpeed  1646 non-null   float64\n",
      " 2   DailyPrecipitation     1704 non-null   object \n",
      " 3   DailySnowDepth         1704 non-null   object \n",
      " 4   DailySnowfall          1704 non-null   object \n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "raw_weather_daily.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_daily_weather(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Cleans the daily weather data by removing rows where all key columns are NaN.\"\"\"\n",
    "    cleaned_df = df.copy()\n",
    "\n",
    "    # Drop rows where all the specified columns are NaN\n",
    "    cleaned_df = cleaned_df.dropna(subset=['DailyAverageWindSpeed', 'DailyPrecipitation', 'DailySnowDepth', 'DailySnowfall'], how='all')\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "e95a3d1d-d062-4095-8761-3fd4eae8654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_daily_weather = clean_daily_weather(raw_weather_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1704 entries, 24 to 54590\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Date                   1704 non-null   object \n",
      " 1   DailyAverageWindSpeed  1646 non-null   float64\n",
      " 2   DailyPrecipitation     1704 non-null   float64\n",
      " 3   DailySnowDepth         1704 non-null   float64\n",
      " 4   DailySnowfall          1704 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 79.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Force conversion to float and replace any non-numeric values with 0\n",
    "cleaned_daily_weather['DailySnowfall'] = pd.to_numeric(cleaned_daily_weather['DailySnowfall'], errors='coerce').fillna(0)\n",
    "cleaned_daily_weather['DailyPrecipitation'] = pd.to_numeric(cleaned_daily_weather['DailyPrecipitation'], errors='coerce').fillna(0)\n",
    "cleaned_daily_weather['DailySnowDepth'] = pd.to_numeric(cleaned_daily_weather['DailySnowDepth'], errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "# Check the info again to verify the column type\n",
    "cleaned_daily_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "341372f5-f251-4fee-bccf-6e15e9bd0b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_daily_avg_wind_speed(hourly_df: DataFrame, daily_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Replaces missing daily average wind speed values in the daily dataset using hourly data.\"\"\"\n",
    "    # Step 1: Calculate daily average wind speed from the hourly data\n",
    "    daily_avg_wind_speed = hourly_df.groupby('Date', as_index=False).agg({\n",
    "        'HourlyWindSpeed': 'mean'  # Calculate mean wind speed for each day\n",
    "    }).rename(columns={'HourlyWindSpeed': 'CalculatedDailyAvgWindSpeed'})\n",
    "    \n",
    "    # Step 2: Merge the daily calculated values into the daily weather DataFrame\n",
    "    merged_df = daily_df.merge(daily_avg_wind_speed, on='Date', how='left')\n",
    "    \n",
    "    # Step 3: Replace missing values in 'DailyAverageWindSpeed' with calculated values\n",
    "    merged_df['DailyAverageWindSpeed'] = merged_df['DailyAverageWindSpeed'].fillna(\n",
    "        merged_df['CalculatedDailyAvgWindSpeed']\n",
    "    )\n",
    "    \n",
    "    # Drop the helper column 'CalculatedDailyAvgWindSpeed' if no longer needed\n",
    "    merged_df = merged_df.drop(columns=['CalculatedDailyAvgWindSpeed'])\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "3f51c9ce-f3cc-4458-bf36-fa200d131327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1704 entries, 0 to 1703\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Date                   1704 non-null   object \n",
      " 1   DailyAverageWindSpeed  1704 non-null   float64\n",
      " 2   DailyPrecipitation     1704 non-null   float64\n",
      " 3   DailySnowDepth         1704 non-null   float64\n",
      " 4   DailySnowfall          1704 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 66.7+ KB\n"
     ]
    }
   ],
   "source": [
    "updated_daily_weather = replace_missing_daily_avg_wind_speed(aggregated_hourly, cleaned_daily_weather)\n",
    "updated_daily_weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3328ef-d547-4832-837f-591190e075b1",
   "metadata": {},
   "source": [
    "Daily data on Sunrise and Sunset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "f5f69ad1-881c-4910-b793-1211ecec900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sunrise_daily(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Processes daily weather data to select relevant columns.\"\"\"\n",
    "    relevant_columns = ['Date','Sunset', 'Sunrise']\n",
    "    return df[relevant_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "c60f35fe-5539-4d8e-baea-cedc1ef51f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunset_sunrise = sunrise_daily(weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "a0d761b2-76b7-4ae6-a70c-2e6b14ccf700",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunset_sunrise = sunset_sunrise.dropna(subset=['Sunset', 'Sunrise'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "afd76ba7-fe0c-4540-9ab8-1f63a47f2c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1704 entries, 24 to 54590\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Date     1704 non-null   object \n",
      " 1   Sunset   1704 non-null   float64\n",
      " 2   Sunrise  1704 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 53.2+ KB\n"
     ]
    }
   ],
   "source": [
    "sunset_sunrise.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "a8ea2d1f-2252-43f9-a5f1-81c29b098794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_time(value):\n",
    "    value = int(value)  # Ensure the value is an integer\n",
    "    hours = value // 100  # Extract hours\n",
    "    minutes = value % 100  # Extract minutes\n",
    "    return f\"{hours:02d}:{minutes:02d}\"  # Format as HH:MM\n",
    "\n",
    "sunset_sunrise = sunset_sunrise.copy()\n",
    "\n",
    "sunset_sunrise[\"Sunset\"] = sunset_sunrise[\"Sunset\"].apply(convert_to_time)\n",
    "sunset_sunrise[\"Sunrise\"] = sunset_sunrise[\"Sunrise\"].apply(convert_to_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "5a561e49-3326-412d-94ae-9f51a2554700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Sunset</th>\n",
       "      <th>Sunrise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>16:39</td>\n",
       "      <td>07:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>16:40</td>\n",
       "      <td>07:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>16:41</td>\n",
       "      <td>07:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>16:42</td>\n",
       "      <td>07:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>16:43</td>\n",
       "      <td>07:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54461</th>\n",
       "      <td>2024-08-27</td>\n",
       "      <td>18:35</td>\n",
       "      <td>05:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54486</th>\n",
       "      <td>2024-08-28</td>\n",
       "      <td>18:33</td>\n",
       "      <td>05:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54518</th>\n",
       "      <td>2024-08-29</td>\n",
       "      <td>18:32</td>\n",
       "      <td>05:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54552</th>\n",
       "      <td>2024-08-30</td>\n",
       "      <td>18:30</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54590</th>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>18:29</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1704 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Sunset Sunrise\n",
       "24     2020-01-01  16:39   07:20\n",
       "49     2020-01-02  16:40   07:20\n",
       "86     2020-01-03  16:41   07:20\n",
       "144    2020-01-04  16:42   07:20\n",
       "169    2020-01-05  16:43   07:20\n",
       "...           ...    ...     ...\n",
       "54461  2024-08-27  18:35   05:19\n",
       "54486  2024-08-28  18:33   05:20\n",
       "54518  2024-08-29  18:32   05:21\n",
       "54552  2024-08-30  18:30   05:22\n",
       "54590  2024-08-31  18:29   05:23\n",
       "\n",
       "[1704 rows x 3 columns]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sunset_sunrise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460bfd8-f5c9-49e5-99cd-4831b022dba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "9d5438b0-c8a0-497d-ae36-5f4ff8831a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rc/sh_trsb91r31xqwtp6547r_r0000gn/T/ipykernel_15008/2984449546.py:12: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Define the database URL\n",
    "DATABASE_URL = 'sqlite:///project.db'\n",
    "\n",
    "# Create an engine instance\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Create a declarative base class\n",
    "Base = declarative_base()\n",
    "\n",
    "# Create a configured \"Session\" class\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "# Create a Session\n",
    "session = Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "f66d6a9e-83c5-4bd5-bc74-9dff52690f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, Float, DateTime, Date\n",
    "\n",
    "class HourlyWeather(Base):\n",
    "    __tablename__ = 'hourly_weather'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    date = Column(Date, nullable=False)\n",
    "    hour = Column(Integer)\n",
    "    hourly_precipitation = Column(Float)\n",
    "    hourly_wind_speed = Column(Float)\n",
    "    hourly_wind_direction = Column(String)\n",
    "\n",
    "class DailyWeather(Base):\n",
    "    __tablename__ = 'daily_weather'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    date = Column(Date, nullable=False)\n",
    "    daily_avg_wind_speed = Column(Float)\n",
    "    daily_precipitation = Column(Float)\n",
    "    daily_snow_depth = Column(Float)\n",
    "    daily_snow_fall = Column(Float)\n",
    "\n",
    "class UberTrip(Base):\n",
    "    __tablename__ = 'uber_trips'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    request_datetime = Column(DateTime)\n",
    "    on_scene_datetime = Column(DateTime)\n",
    "    pickup_datetime = Column(DateTime)\n",
    "    dropoff_datetime = Column(DateTime)\n",
    "    trip_miles = Column(Float)\n",
    "    trip_time = Column(Float)\n",
    "    base_passenger_fare = Column(Float)\n",
    "    tolls = Column(Float)\n",
    "    black_car_fund = Column(Float)  \n",
    "    sales_tax = Column(Float)\n",
    "    congestion_surcharge = Column(Float)\n",
    "    airport_fee = Column(Float)\n",
    "    tips = Column(Float)\n",
    "    driver_pay = Column(Float)\n",
    "    shared_request_flag = Column(Integer)  # Binary Variable\n",
    "    shared_match_flag = Column(Integer)    # Binary Variable\n",
    "    access_a_ride_flag = Column(Integer)   # Binary Variable\n",
    "    wav_request_flag = Column(Integer)     # Binary Variable\n",
    "    wav_match_flag = Column(Integer)       # Binary Variable\n",
    "    pickup_latitude = Column(Float)\n",
    "    pickup_longitude = Column(Float)\n",
    "    dropoff_latitude = Column(Float)\n",
    "    dropoff_longitude = Column(Float)\n",
    "\n",
    "class YellowTaxiTrip(Base):\n",
    "    __tablename__ = 'yellow_taxi_trips'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    vendorid = Column(Integer)\n",
    "    pickup_datetime = Column(DateTime)\n",
    "    dropoff_datetime = Column(DateTime)\n",
    "    passenger_count = Column(Integer)\n",
    "    trip_distance = Column(Float)\n",
    "    rate_code_id = Column(Integer)  \n",
    "    store_and_fwd_flag = Column(Integer)  # Binary Variable\n",
    "    payment_type = Column(Integer)\n",
    "    fare_amount = Column(Float)\n",
    "    miscellaneous_extras = Column(Float)  \n",
    "    mta_tax = Column(Float)\n",
    "    tip_amount = Column(Float)\n",
    "    tolls_amount = Column(Float)\n",
    "    improvement_surcharge = Column(Float)\n",
    "    total_amount = Column(Float)\n",
    "    congestion_surcharge = Column(Float)\n",
    "    airport_fee = Column(Float)\n",
    "    pickup_latitude = Column(Float)\n",
    "    pickup_longitude = Column(Float)\n",
    "    dropoff_latitude = Column(Float)\n",
    "    dropoff_longitude = Column(Float)\n",
    "\n",
    "class Sun_Data(Base):\n",
    "    __tablename__ = 'sun_data'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    Date = Column(Date)\n",
    "    Sunrise = Column(DateTime)\n",
    "    Sunset = Column(DateTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "aa00b054-3c17-4cc9-b10b-83ab14adc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all tables in the database\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "9ae1e4c5-ed0a-4b4a-9f54-f24a7f9859fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Uber trips dataset\n",
    "df_uber = pd.read_parquet('Cleaned_Uber_Data/Uber_all.parquet')\n",
    "\n",
    "# Load the Yellow Taxi trips dataset\n",
    "df_taxi = pd.read_parquet('Cleaned_Taxi_Data/Taxi_all.parquet')\n",
    "\n",
    "# Assuming aggregated_hourly and updated_daily_weather are functions\n",
    "# that return DataFrames for hourly and daily weather data respectively\n",
    "df_hourly_weather = aggregated_hourly\n",
    "df_daily_weather = updated_daily_weather\n",
    "df_sunset_sunrise = sunset_sunrise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "d846e6fd-ea9a-4a07-bc12-c7f0413d937f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1704"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert data into the Uber trips table\n",
    "df_uber.to_sql('uber_trips', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Insert data into the Yellow Taxi trips table\n",
    "df_taxi.to_sql('yellow_taxi_trips', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Insert data into the Hourly Weather table\n",
    "df_hourly_weather.to_sql('hourly_weather', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Insert data into the Daily Weather table\n",
    "df_daily_weather.to_sql('daily_weather', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "df_sunset_sunrise.to_sql('daily_weather', con=engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69b8c6-c21d-4bb3-b479-03d38e4e146f",
   "metadata": {},
   "source": [
    "## Create a SQL Schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "eea4e269-26a8-4c06-9447-568c852e9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.schema import CreateTable\n",
    "\n",
    "# Open a file to write the schema\n",
    "with open('schema.sql', 'w') as f:\n",
    "    for table in Base.metadata.sorted_tables:\n",
    "        create_table_sql = str(CreateTable(table).compile(engine))\n",
    "        f.write(f\"{create_table_sql};\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geopandas_env)",
   "language": "python",
   "name": "geopandas_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
